%    Sidebar about panic:
% 	panic is the kernel's last resort: the impossible has happened and the
% 	kernel does not know how to proceed.  In xv6, panic does ...
\chapter{Traps and device drivers}
\label{CH:TRAP}

There are three situations in which some event causes the CPU to set
aside its ordinary sequential execution of instructions and forces a
transfer of control to special code that handles the event. One
situation is a system call, when a user program 
executes the {\tt ecall} instruction to ask the kernel to do 
something for it. Another situation is an \indextext{exception}:
an instruction (user or kernel) does something illegal, such as divide
by zero or use an invalid virtual address. The third situation is a
device \indextext{interrupt}, when a device signals that it needs
attention, for example when the disk hardware finishes a read or write
request.

This book uses \indextext{trap} as a generic term for these
situations. Typically whatever code was executing at the time of the
trap will later need to resume, and shouldn't need to be aware that
anything special happened. That is, we often want traps to be
transparent; this is particularly important for interrupts, which the
interrupted code typically doesn't expect. Thus the usual sequence is that
a trap forces a transfer of control into the kernel; the kernel saves
registers and other state so that execution can be resumed; the kernel
executes appropriate handler code (e.g. a system call implementation
or device driver); the kernel restores the saved state and returns
from the trap; and the original code resumes where it left off.

The xv6 kernel handles all traps. This is natural for system calls; it
makes sense for interrupts since isolation demands that user processes
not directly use devices, and because only the kernel has the state
needed for device handling; and it makes sense for exceptions since
xv6 responds to all exceptions from user space by killing the
offending program.

Xv6 trap handling proceeds in four stages: hardware actions taken by
the RISC-V CPU, an assembly ``vector'' that prepares the way for
kernel C code, a C trap handler that decides what to do with the trap,
and the system call or device driver service routine. While
commonality among the three trap types suggests that a kernel could
handle all traps with a single code path, it turns out to be
convenient to have separate assembly vectors and C trap handlers for
three distinct cases: traps from kernel space, traps from user space,
and timer interrupts.

This chapter ends with a discussion of device drivers. Device handling
is a different topic than traps, but is included here because the
kernel's interaction with device hardware is often driven by
interrupts.

\section{RISC-V trap machinery}

RISC-V supports a number of control registers that the kernel writes to
tell the CPU how to handle interrupts, and that the kernel can read
to find out about an interrupt that has occured. The RISC-V documents
contain the full story~\cite{riscv:priv}. {\tt riscv.h}
\lineref{kernel/riscv.h:1} contains definitions that xv6 uses. Here's
an outline of the most important registers:

\begin{itemize}

\item {\bf stvec}: The kernel writes the address of its trap handler
  here; the RISC-V jumps here to handle a trap.

\item {\bf sepc}: When a trap occurs, RISC-V saves the program counter
  here (since the {\tt pc} is then replaced with {\tt stvec}). The
  {\tt sret} (return from trap) instruction copies {\tt sepc} to the
  {\tt pc}. The kernel can write to {\tt sepc} to control where {\tt
    sret} goes.

\item {\bf scause}: The RISC-V puts a number here that describes
the reason for the trap.

\item {\bf sscratch}: The kernel places a value here that comes in
  handy at the very start of a trap handler.

\item {\bf sstatus}: The SIE bit controls whether device interrupts
  are enabled. If the kernel clears SIE, the RISC-V will defer
  device interrupts until the kernel sets SIE. The SPP bit
  indicates whether a trap came from user mode or supervisor
  mode, and controls to what mode {\tt sret} returns.

\end{itemize}

The above relate to interrupts handled in supervisor mode, and they
cannot be read or written in user mode. There is an equivalent set of
control registers for interrupts handled in machine mode; xv6 uses
them only for the special case of timer interrupts.

When it needs to force a trap, the RISC-V hardware does the
following:

\begin{enumerate}

\item If the trap is a device interrupt, and the {\tt sstatus} SIE bit
  is clear, don't do any of the following.

\item Copy the {\tt pc} to {\tt sepc}.

\item Save the current mode (user or supervisor) in the SPP bit in {\tt sstatus}.

\item Set {\tt scause} to reflect the interrupt's cause.

\item Set the mode to supervisor.

\item Copy {\tt stvec} to the {\tt pc}.

\item Start executing at the new {\tt pc}.

\end{enumerate}

It is important that the CPU performs all these steps as
a single operation.
Consider if one of these steps were omitted: for example, the CPU
didn't switch program counters. Then, a trap could switch to
supervisor mode while still running user instructions. Those user
instructions could break the user/kernel isolation, for example by
modifying the {\tt satp} register to point to a page table that
allowed accessing all of physical memory. It is thus important that
the kernel specify the trap entry point, and not the user program.

Note that the CPU doesn't switch to the kernel page table, doesn't
switch to a stack in the kernel, and doesn't save any registers other
than the {\tt pc}. The kernel must perform these tasks if necessary.
One reason that the CPU does minimal work during a trap is to provide
flexibility to software; for example, in some situations no page table
switch is needed, which may increase performance.

\section{Kernel trap vectors}

Because the RISC-V hardware has minimal mechanism for handling traps,
much of the work of entering and leaving the kernel is done by kernel
software. The next stage of trap handling is done in assembly code to
set up an environment in which it is safe to run C code; then the
assembly code jumps to C trap handling code. There are two main cases
(again ignoring timer interrupts):

\begin{itemize}

\item Traps from supervisor mode. These are traps that occur while the
  xv6 kernel is executing. In this case the trap handler can rely on
  {\tt satp} already being set to the kernel page table, and the stack
  pointer already pointing to a valid stack in the kernel. The
  assembly handler code pushes the current registers onto the
  stack, and jumps to C code.

\item Traps from user mode. This case is challenging, since {\tt satp}
  points to a user page table that has no mappings for the kernel, and
  the stack pointer may contain an invalid or even malicious value.

  To addresses these challenges, xv6 uses a \indextext{trampoline}
  page.  It maps a page with code to enter and exit the kernel in
  every process's address space and in the kernel address space at the
  address \indexcode{TRAMPOLINE} (as we saw in Figure~\ref{fig:as} and
  in Figure~\ref{fig:xv6_layout}).  This page is mapped at the same
  address in each address space.  This page contains the code that
  switches page tables, and because the page is mapped identically in
  each page table, the instruction right after the page table switch
  will translate identically.  The trampoline code for entering needs
  access to some area to save and restore CPU state, and therefore xv6
  also conveniently maps the process's trapframe (\lstinline{p->tf})
  in each process address space at \indexcode{TRAPFRAME}, right below
  \lstinline{TRAMPOLINE}.  The trampoline also switches from a user
  stack to a valid kernel stack.
  
\item Timer interrupts.  Timer interrupts can occur in user
  or supervisor mode, and always switch to machine mode.
  Because machine mode does not use the paging hardware,
  xv6 restricts itself to a very simple timer interrupt handler.
  It resets the
  timer, sets a bit in the \lstinline{sip} register to ask
  for a software interrupt to be delivered in supervisor mode,
  and returns from the
  interrupt to whatever mode it came from (supervisor or user mode).
  Because there is an interrupt pending for supervisor mode, the
  CPU immediately will take it unless interrupts are disabled.

\end{itemize}

A small hurdle for trap handlers is that they often
need some scratch space. When the trap handler starts it cannot
use any register because those contain values of the interrupted code
and they must be saved before the trap handler can use them. But,
to save them, it needs a register.  To handle this conundrum,
RISC-V provides a scratch register for machine and kernel
mode: \indexcode{mscratch} and \indexcode{sscratch}, respectively.
RISC-V also has an instruction to exchange the content
of a register with the content of the scratch register. After
executing this instruction, the register will contain the address of
the scratch area and the scratch register the content of the
register. The trap handler can now use the register to save other
registers in the scratch area.

The trap handling code needs to know why the trap occured.
RISC-V provides a register
\indexcode{scause}, which specifies the cause of the trap (e.g.,
timer interrupt, illegal instruction, system call).

\section{Code: traps from supervisor mode}

Exceptions and interrupts (other than the timer) can occur while
executing in the kernel in supervisor mode. The hardware reacts by
saving the {\tt pc} in {\tt sepc}, and jumping to an xv6 trap handler
addressed by {\tt stvec}. At a high level, xv6 then pushes the current
registers onto the current (kernel) stack, handles the trap, restores
registers by popping them from the stack, and uses {\tt sret} to resume.

The details are as follows. During start-up xv6 stores the address of
\lstinline{kernelvec} in the trap vector register for supervisor mode
(the \lstinline{stvec} register)
\lineref{kernel/trap.c:/trapinithart/}.  \lstinline{kernelvec} saves
all registers on the current stack
\linerefs{kernel/kernelvec.S:/sd ra/,/sd t6/}
and then calls the C function \lstinline{kerneltrap}
\lineref{kernel/kernelvec.S:/call/}.  \lstinline{kerneltrap}
\lineref{kernel/trap.c:/kerneltrap/} performs a few sanity checks
(e.g., it checks that it was indeed interrupted in supervisor mode) and
then yields the core so that another process can run.  We will discuss
the implementation of \lstinline{yield} in Chapter~\ref{CH:SCHED}.

At some point later, the scheduler may decide to resume the process
that was interrupted by the timer interrupt. The process returns from
\lstinline{yield}, perhaps running now on a different core.  It
restores \lstinline{sepc} register and \lstinline{sstatus} register.
\lstinline{kerneltrap} returns and restores all the saved registers
\linerefs{kernel/kernelvec.S:/ld ra/,/ld t6/}, adjusts the stack
pointer, and resumes at the addresses stored in \lstinline{sepc},
which is the address of the instruction that was interrupted.

\section{Code: traps from user mode}

The function \lstinline{usertrapret}
\lineref{kernel/trap.c:/usertrapret/} sets up the trampoline to leave
and enter the kernel.  It starts out by disabling interrupts
\lineref{kernel/trap.c:/intr_off/}; xv6 is switching from taking
interrupts from \lstinline{kerneltrap} to \lstinline{usertrap} and
during this switch xv6 is not ready to take interrupts.

Next, \lstinline{usertrapret} sets the address where to enter the
kernel (by writing \lstinline{uservec} into \lstinline{stvec}) and it
sets up the values the trampoline needs when entering the kernel: the
kernel page table, the kernel stack of this process, the function to
call, and the hartid
\linerefs{kernel/trap.c:/kernel_satp.=.r_satp/,/r_tp/}.  We will see
how these values are used when entering the kernel.

Then, \lstinline{usertrapret} sets up values the trampoline needs to
exit the kernel and return to user space.  It will set the
previous mode in the \lstinline{sstatus} register to user mode (0) and
enable user interrupts in user mode, it will set the \lstinline{sepc}
register to the address the user program should be resumed at, and
will set the \lstinline{satp} register to this process's page table
\linerefs{kernel/trap.c:/x.=.r_sstatus/,/p->pagetable/}.
Finally, \lstinline{usertrapret} calls
the trampoline function \lstinline{userret}, passing two arguments: the addresss
\lstinline{TRAPFRAME} and the value for \lstinline{satp}.

\lstinline{userret} \lineref{kernel/trampoline.S:/userret/} loads
\lstinline{satp} with the user page table.  After this instruction,
the CPU will translate virtual address with the user page table;
for example, the address of the next instruction will be translated
with the user page table.  This address will be translated in the same
way as with the kernel page table, because the mapping is the same in
both page tables.

To execute the remaining instructions, \lstinline{userret} wants to
use register (\lstinline{a0}), which contains the first argument to
\lstinline{userret}, \lstinline{TRAPFRAME}, the process's
trapframe. But, it must also restore \lstinline{a0} to the user
process's value for \lstinline{a0}, which is stored in the process's
trapframe \lstinline{p->tf}.  The kernel stores the to-be-restored value for
\lstinline{a0} in \lstinline{sscratch} and later restores
\lstinline{a0} from there \lineref{kernel/trampoline.S:/csrrw a0/}.

Now \lstinline{a0} is availabe, it uses it to restore all other CPU
registers from TRAPFRAME \linerefs{kernel/trampoline.S:/ld ra/,/ld t6/}.
Note that the user page table, which we just installed, maps
TRAPFRAME to \lstinline{p->tf}.  Thus, even though \lstinline{a0}
contains \lstinline{TRAPFRAME}, we are loading values from
\lstinline{p->tf}.  To make sure that user code cannot read/write
\lstinline{p->tf} through \lstinline{TRAPFRAME}, xv6 maps
\lstinline{TRAPFRAME} with the \lstinline{PTE_U} bit clear
\linerefs{kernel/proc.c:/TRAPFRAME/,/./}.

Then, \lstinline{userret} restores \lstinline{a0} by switching the
values of \lstinline{sscratch} and \lstinline{a0}. After this
instruction, \lstinline{a0} contains the user process's \lstinline{a0}
and \lstinline{sscratch} contains \lstinline{p->tf}.  Having
\lstinline{p->tf} in \lstinline{sscratch} will be convenient when
entering: this is the address where xv6 will save registers.  Finally,
\lstinline{userret} calls \lstinline{sret}, which returns to the
previous mode (user mode) with interrupts enabled and resuming at the
address in \lstinline{sepc}.  We are out of the kernel.

If a timer interrupt happens, the CPU will switch to supervisor mode with
the program counter set to the value in \lstinline{stvec} (i.e.,
\lstinline{uservec}) and \lstinline{sepc} containing the address of
the instruction that was interrupted.  \lstinline{uservec} first saves
\lstinline{a0} and loads \lstinline{TRAPFRAME}, which maps to
\lstinline{p->tf}, into \lstinline{a0}
\lineref{kernel/trampoline.S:/csrrw a0/}.
Next, it saves all other
CPU registers into \lstinline{p->tf}
\linerefs{kernel/trampoline.S:/sd ra/,/sd t6/}.

Now all registers are saved, \lstinline{uservec} can
use \lstinline{t0} to save the original value of \lstinline{a0}.  It
switches to the kernel stack of the process, because the user stack
may not be valid.  It switches to the kernel page table; again, this
works out because trampoline page is mapped identically in the user
and kernel address space.  Finally, it calls \lstinline{usertrap} (its
address is in \lstinline{t0}).  We are back in the kernel with a
kernel page table and on a kernel stack, running C code.

\section{Code: C trap handler}

The \lstinline{usertrap} function changes \lstinline{stvec} to the
address of \lstinline{kerneltrap} because xv6 is in supervisor mode and
when xv6 enables interrupts in supervisor mode, interrupts should go
through \lstinline{kerneltrap}; there is no need to switch page tables
and stacks. \lstinline{usertrap} next saves the \lstinline{sepc}
because that register contains the instruction where the process
should be resumed after completing the trap.
If the user process entered the kernel to execute a system
call, the register \lstinline{scause} contains 8.  In that case, xv6
adds 4 to \lstinline{p->tf->epc}, to skip the instruction that entered
the kernel (\lstinline{ecall}) so that on return to user space the
process will resume at the instruction after \lstinline{ecall}.
\lstinline{usertrap} then enables interrupts and calls
\lstinline{syscall}.

If an exception causes the kernel/user transition, then xv6 records
that the user process must be killed and will call \lstinline{exit}.
(We will look at how xv6 does this cleanup in Chapter~\ref{CH:SCHED}.)
Otherwise, the cause was an interrupt, which are dealt with by
\indexcode{devintr}.  For a timer
interrupt, \lstinline{devintr} just do two things: increment the ticks
variable \lineref{kernel/trap.c:/ticks\+\+/}, and call
\indexcode{wakeup}.  The latter, as
we will see in Chapter~\ref{CH:SCHED}, may make a process runnable.

After handling the trap, xv6 yields
the core to another process if the trap was a timer interrupt.
Otherwise, \lstinline{usertrap} exits the kernel and returns back to
user space using \lstinline{usertrapret}, which was explained above.
We have seen all the machinery to enter and leave the kernel.

\section{Code: timer interrupts}

By default the RISC-V executes all trap handlers in machine mode.  xv6 programs
RISC-V to delegate all interrupts and exceptions to supervisor mode
\linerefs{kernel/start.c:/w_medeleg/,/w_mideleg/}.
Timer interrupts, however, must be received in machine
mode. Therefore, xv6 programs the hardware to receive timer interrupts in
machine mode: it stores into the \lstinline{mtvec} register the
address to jump to on an interrupt \lineref{kernel/start.c:/w_mtvec/}
and enables timer interrupts in the \lstinline{mie} register
\lineref{kernel/start.c:/w_mie/}.

When the CLINT interrupts the CPU, the CPU switches its
program counter to the value stored in \lstinline{mtvec} register,
which is \lstinline{machinevec}.  \lstinline{machinevec} is defined in
assembly \lineref{kernel/kernelvec.S:/machinevec/}.
\lstinline{machinevec} starts out by exchanging \lstinline{a0} and the
\lstinline{mscratch} register.  xv6 stored earlier in the
\lstinline{mscratch} register \lineref{kernel/start.c:/mscratch/} the
address \lstinline{mscratch0}, which points to an area of
\lstinline{uint64}s.  Entries 4 and 5 contain the address of the
core's \lstinline{CLINT_MTIME} and the new value
\linerefs{kernel/start.c:/scratch.4/,/scratch.5/}.
\lstinline{machinevec} saves registers \lstinline{a1} through
\lstinline{a3} in the scratch area so that it can use them for its own
computation.  Then, it reprograms \lstinline{CLINT_MTIMECMP} to
generate an interrupt in 10ms and sets the interrupt bit in the kernel
\lstinline{sip} register.  Setting this bit will cause the kernel to
receive an interrupt after \lstinline{machinevec} completes.
\lstinline{machinevec} completes by restoring the registers it used
and returning from machine mode \lineref{kernel/kernelvec.S:/mret/} to
the mode where it came from when it was interrupted.

The code that was interrupted cannot tell it was interrupted: the
CPU is in exactly the same state as before the interrupt,
because the \lstinline{machinevec} has restored all registers.  This
idea of saving the CPU state to run some code, and then
restoring the CPU state to resume the original code is a pattern
we will see a few more times.
 
\section{Code: Enabling/disabling interrupts}

A CPU can control if it wants to receive external and timer
interrupts through the \lstinline{SIE_SEIE}
\index{SIE_SEIE@\lstinline{SIE_SEIE}} flag and \lstinline{SIE_STIE}
\index{SIE_STIE@\lstinline{SIE_STIE}} flag in the \texttt{sie}
register, respectively.  The bootloader disables interrupts during
booting of the main cpu and the other other CPUs.  The scheduler on
each CPU enables interrupts \lineref{kernel/proc.c:/intr_on/}.  To
control that certain code fragments are not interrupted, xv6 disables
interrupts during these code fragments.  For example,
\lstinline{usertrapret} above clears interrupts
\lineref{kernel/trap.c:/intr_off/}.

\section{Code: System calls}

Chapter~\ref{CH:FIRST} ended with 
\indexcode{initcode.S}
invoking a system call.
Let's look at that again
\lineref{user/initcode.S:/SYS_exec/}.
The process pushed the arguments
for an 
\indexcode{exec}
call on the process's stack, and put the
system call number in
\texttt{a7}.
The system call numbers match the entries in the syscalls array,
a table of function pointers
\lineref{kernel/syscall.c:/syscalls/}.
The \lstinline{ecall} instruction
switches the CPU from user mode to supervisor mode, and will
cause the kernel to call \lstinline{syscall}, as we saw above.

\indexcode{Syscall}
\lineref{kernel/syscall.c:/^syscall/} 
loads the system call number from the trapframe, which
contains the saved
\texttt{a7},
and indexes into the system call tables.
For the first system call, 
\texttt{a7}
contains the value 
\indexcode{SYS_exec}
\lineref{kernel/syscall.h:/SYS_exec/},
and
\lstinline{syscall}
will invoke the 
\lstinline{SYS_exec} 'th 
entry of the system call table, which corresponds to invoking
\lstinline{sys_exec}.

\lstinline{Syscall}
records the return value of the system call function in
\lstinline{p->tf->a0}.
When the system call returns to user space,
\lstinline{usertrapret}
will load the values
from
\indexcode{p->tf}
into the machine registers
and return to user space
using
\lstinline{sret}.
Thus, when 
\lstinline{exec}
returns, it will return in \lstinline{a0} the value
that the system call handler returned
\lineref{kernel/syscall.c:/a0 = syscalls/}.
System calls conventionally return negative numbers to indicate
errors, positive numbers for success.
If the system call number is invalid,
\indexcode{syscall}
prints an error and returns \-1.

\section{Code: System call arguments}

Later chapters will examine the implementation of
particular system calls.
This chapter is concerned with the mechanisms for system calls.
There is one bit of mechanism left: finding the system call arguments.
The helper functions
\lstinline{argint},
\lstinline{argaddr},
\lstinline{argptr},
\lstinline{argstr},
and
\lstinline{argfd}
retrieve the 
\textit{n} 'th 
system call
argument, as either an integer, pointer, a string, or a file descriptor.
\indexcode{argint}
and
\indexcode{argaddr}
use the function
\lstinline{fetcharg}
to locate the
\textit{n}'th 
argument. The C calling conventions specify that argument 0 is passed
through
\texttt{a0},
argument 1 through
\texttt{a1}, ...,
argument 6 through
\texttt{a6}.

\lstinline{argint} calls \indexcode{fetchint} to read the value at
that address from user memory and write it to \lstinline{*ip}.
\lstinline{fetchint} cannot simply cast the address to a pointer,
because the user and the kernel don't share the same page
table. Instead, \lstinline{fetchint} calls \lstinline{copyin} to copy
the value at the address from the process's address space to the
kernel address space.  Before calling \lstinline{copyin},
\lstinline{fetchint} verifies that the address lies in the process's
addres space: it checks that the address is below \indexcode{p->sz}.

\indexcode{copyin}
\lineref{kernel/vm.c:/^copyin/} copies \lstinline{len} bytes to
\lstinline{dst} from virtual address \lstinline{srcva} in the page
table \lstinline{pagetable}.  It walks the page table in software to
determine the physical address \lstinline{pa0} for \lstinline{srcva}.
Since the kernel maps virtual address one-to-one on physical
addresses and maps all physical addresses, the kernel can use
\lstinline{pa0} as a virtual address and copy the bytes at
\lstinline{pa0} using \lstinline{memmove}.

\indexcode{fetchaddr},
is like
\lstinline{fetchint},
but retrieves 64-bit value instead of a 32-bit int.

\indexcode{argptr}
fetches the
\textit{n}'th 
system call argument and checks that this argument is a valid
user-space pointer.

\indexcode{argstr} 
interprets the
\textit{n}'th 
argument as a pointer.  It ensures that the pointer points at a
NUL-terminated string and that the complete string is located below
the end of the user part of the address space.

Finally,
\indexcode{argfd}
\lineref{kernel/sysfile.c:/^argfd/}
uses
\lstinline{argint}
to retrieve a file descriptor number, checks if it is valid
file descriptor, and returns the corresponding
\lstinline{struct}
\lstinline{file}.

The system call implementations (for example, in sysproc.c and sysfile.c)
are typically wrappers: they decode the arguments using 
\lstinline{argint},
\lstinline{argaddr},
\lstinline{argptr}, 
and 
\lstinline{argstr}
and then call the real implementations.
In Chapter~\ref{CH:MEM},
\lstinline{sys_exec}
uses these functions to get at its arguments.

\section{RISC-V Device Interrupts}

Devices on the RISC-V development board can generate interrupts, and
xv6 must set up the hardware to handle these interrupts.  Devices
usually interrupt in order to tell the kernel that some hardware event
has occured, such as I/O completion.  Interrupts are usually optional
in the sense that the kernel could instead periodically check (or
\indextext{poll}) the device hardware to check for new
events.  Interrupts are preferable to polling if the events are
relatively rare, so that polling would waste CPU time.

Devices can generate interrupts
at any time.  There is hardware on the board to signal the CPU
when a device needs attention (e.g., the user has typed a character on
the keyboard). We must program the device to generate an interrupt, and
arrange that a CPU receives the interrupt. 

Let's look at the timer device and timer interrupts.  We would like
the timer hardware to generate an interrupt, say, 10 times per
second so that the kernel can track the passage of time and so the
kernel can time-slice among multiple running processes.  The choice of
10 times per second allows for decent interactive performance while
not swamping the CPU with handling interrupts.

RISC-V on which xv6 is running has a \textit{Core Local
  Interruptor (CLINT)}\index{Core Local Interruptor (CLINT)}~\cite{u54}, which
can be programmed to generate timer interrupts.  The CLINT is a
memory-mapped device located at \lstinline{0x02000000} (see
Fig.~\ref{fig:xv6_layout}), and can thus be programmed with ordinary
load and store instructions. The CLINT exposes two registers relating
to timer interrupts: {\tt MTIME} is a running count of the number of
cycles (at a machine-dependent rate) since boot, and {\tt MTIMECMP}
holds, per CPU, the future {\tt MTIME} value at which the CPU
would like a timer interrupt.
xv6 configures the CLINT in {\tt start.c}
\lineref{kernel/start.c:/MTIMECMP/}
to generate a timer interrupt
every million cycles, which turns out to be about ten times
per second. In a bit we'll discuss what happens when those
interrupts arrive.

\section{Drivers}

A
\indextext{driver}
is the code in an operating system that manages a particular device:
it tells the device hardware to perform operations,
configures the device to generate interrupts when done,
and handles the resulting interrupts.
Driver code can be tricky to write
because a driver executes concurrently with the device that it manages.  In
addition, the driver must understand the device's interface (e.g., which I/O
ports do what), and that interface can be complex and poorly documented.

The disk driver provides a good example.  The disk driver copies data
from and back to the disk.  Disk hardware traditionally presents the data on the
disk as a numbered sequence of 512-byte 
\textit{blocks} 
\index{block}
(also called 
\textit{sectors}): 
\index{sector}
sector 0 is the first 512 bytes, sector 1 is the next, and so on. The block size
that an operating system uses for its file system maybe different than the
sector size that a disk uses, but typically the block size is a multiple of the
sector size. To
represent a block xv6 has a structure
\lstinline{struct buf}
\lineref{kernel/buf.h:/^struct.buf/}.
The
data stored in this structure is often out of sync with the disk: it might have
not yet been read in from disk (the disk is working on it but hasn't returned
the sector's content yet), or it might have been updated but not yet written
out.  The driver must ensure that the rest of xv6 doesn't get confused when the
structure is out of sync with the disk.

\section{Code: Disk driver}

Xv6 has a disk driver for \indextext{virtio} disks. The
virtio standard is a standard for devices for kernels that run on a
virtual machine~\cite{virtio}.  It defines the interactions between a guest kernel
and the virtual machine monitor for virtual devices emulating network
devices, disk devices, etc.  Xv6 runs on \texttt{qemu} and
\texttt{qemu} supports a virtio disk device for RISC-V platforms.  The
base development board that xv6 targets (SiFive's HiFive) doesn't
provide a disk.

Xv6 represent file system blocks using
\indexcode{struct buf}
\lineref{kernel/buf.h:/^struct.buf/}.
\lstinline{BSIZE}
\lineref{kernel/fs.h:/BSIZE/}
is twice the virtio disk's sector size and thus
each buffer represents the contents of two sectors on a particular
disk device.  The
\lstinline{dev}
and
\lstinline{blockno}
fields give the device and block
number and the
\lstinline{data}
field is an in-memory copy of the disk sectors that correspond to block.
Operating systems often use
bigger blocks than 1,024 bytes to obtain higher disk throughput.

The
\lstinline{flags}
track the relationship between memory and disk:
the
\indexcode{B_VALID}
flag means that
\lstinline{data}
has been read in, and
the 
\indexcode{B_DIRTY} 
flag means that
\lstinline{data}
needs to be written out.

Like programming the CLINT, programming the virtio disk is done
through memory-mapped I/O.  The virtio disk is at address
\lstinline{0x10001000} in the kernel adddres space (see
Figure~\ref{fig:xv6_layout}).  The driver uses ordinary load and
stores instructions to interact with the device, but the addresses
have special meaning.  For example, reading from the address
\lstinline{0x10001000 + VIRTIO_MMIO_VERSION}
\lineref{kernel/virtio.h:/VERSION/} returns the version of the virtio
standard that the device supports
\lineref{kernel/virtio\_disk.c:/MMIO_VERSION/}.  The driver uses the
macro \lstinline{R} to read/write registers on the disk
\lineref{kernel/virtio\_disk.c:/R/}.

The kernel initializes the disk driver at boot time by calling
\indexcode{virtio_disk_init}
\lineref{kernel/virtio\_disk.c:/^virtio_disk_init/} from
\indexcode{main} \lineref{kernel/main.c:/virtio_disk_init/}.
\lstinline{virtio_disk_init} negotiates the simplest features with the
disk \linerefs{kernel/virtio\_disk.c:/features/,/=.features/} and sets
up one queue for disk requests \linerefs{kernel/virtio\_disk.c:/QUEUE_SEL/,/free\[/}.  The driver can have
\lstinline{NUM} outstanding requests to the disk.

Disk accesses typically take milliseconds, a long time for a
CPU.  Therefore, xv6 lets another process run on the CPU after a
disk I/O request and arranges to receive an interrupt when the disk
operation has completed.  
\indexcode{plic_init}, which is called from \lstinline{main}, programs
the \indextext{Platform Level Interrupt Controller (PLIC)}~\cite{riscv:priv} to
accept interrupts from the disk.  It enables interrupts for
\lstinline{VIRTIO0_IRQ} \lineref{kernel/plic.c:/^plicinit/}.  If the
disk generates an interrupt, \lstinline{devintr}
\lineref{kernel/trap.c:/^devintr/} will see that
\lstinline{VIRTIO0_IRQ} was enabled and invoke
\lstinline{virtio_disk_intr}.

After this setup, the disk is not used again until the buffer cache calls
\indexcode{virtio_disk_rw}
\lineref{kernel/virtio\_disk.c:/^virtio_disk_rw/},
which updates a locked buffer
as indicated by the flags.
If
\indexcode{B_DIRTY}
is set,
\lstinline{virtio_disk_rw}
writes the buffer
to the disk; if
\indexcode{B_VALID}
is not set,
\indexcode{virtio_disk_rw}
reads the buffer from the disk.
To add a request to the queue of requests, the driver must find 3
descriptors to describe the request.  The first descriptor
describes the operation (e.g., read/write).  The second descriptor
records the address of the data to be read or written. The third
descriptor is where the device will post the result status of the
request.  \lstinline{Virtio_Disk_Rw} starts by finding 3 free descriptors.  If
it cannot find 3 descriptors, it means that the queue of requests is
full, and it waits until a previously issued request completes, freeing
up descriptors.   If it finds 3 descriptors, the drivers fills them
in \linerefs{kernel/virtio\_disk.c:/buf0.type/,/.next.=.0/}.

The virtio disk supports \textit{Direct Memory Access
  (DMA)}\index{DMA}, which allows the device to read/write physical
memory.  The driver gives the device the physical address of the
buffer's data \lineref{kernel/virtio\_disk.c:/b->data/} and the device
copies directly to or from main memory.  DMA is faster and more
efficient than programmed I/O and is less taxing for the CPU's memory
caches.

Once the driver has filled out the descriptors, it can alert the disk
that a new quests is available.  It does so by writing to
\lstinline{avail}.  The disk monitors \lstinline{avail} and reads the
value in \lstinline{avail[1]} to find out how many new requests it
should process.  To make sure that writes to \lstinline{avail} are not
re-ordered, the driver inserts a memory barrier between them
\lineref{kernel/virtio\_disk.c:/sync_synchronize/}. This guarantees
that if the disk sees a new requests in \lstinline{avail[1]}, it will
also see the correct index in \lstinline{avail[2 + (avail[1]%NUM)]}.

Having posted the request to the disk,
\lstinline{virtio_disk_rw}
must wait for the result.  As discussed above,
polling does not make efficient use of the CPU.
Instead,
\indexcode{virtio_disk_rw}
yields the CPU for other processes by sleeping,
waiting for the interrupt handler to 
record in the buffer's flags that the operation is done
\linerefs{kernel/virtio\_disk.c:/while.*VALID/,/sleep/}.
While this process is sleeping,
xv6 will schedule other processes to keep the CPU busy.

Eventually, the disk will finish its operation and trigger an
interrupt, which will result in a call to
\indexcode{virtio_disk_intr}
to handle it
\lineref{kernel/trap.c:/virtio_disk_intr/}.
The disk posts the requests that it has completed in \lstinline{used}.
\lstinline{Virtio_Disk_Intr}
\lineref{kernel/virtio\_disk.c:/^virtio_disk_intr/}
reads \lstinline{used} to find the \lstinline{id} of
the request that has completed.
Then, \lstinline{virtio_disk_intr}
sets 
\indexcode{B_VALID},
clears
\indexcode{B_DIRTY} for the buffer in the request,
wakes up any process sleeping on the buffer
\linerefs{kernel/virtio\_disk.c:/info\[id\]\.b/,/wakeup/},
and frees up the descriptors that were used for request.
Freeing up a descriptor also wakes up processes
waiting for free descriptors.

The PLIC routes interrupts to the first CPU that claims it.
Thus, cores can process different disk interrupts concurrently.  In
some operating systems, this routing can get sophisticated.  For
example, a network driver might arrange to deliver interrupts for
packets of one network connection to the CPU that is managing
that connection, while interrupts for packets of another connection
are delivered to another CPU.  If some network connections are
short lived while others are long lived and the operating system wants
to keep all CPUs busy to achieve high throughput, this routing
becomes quite complex.

\section{Real world}

Supporting all the devices on a typical computer in its full glory is
much work, because there are many devices, the devices have many
features, and the protocol between device and driver can be complex
and badly documented.
In many operating systems, the drivers together account for more code
in the operating system than the core kernel.

Actual device drivers are far more complex than the disk driver in this chapter,
but the basic ideas are the same:
typically devices are slower than CPU, so the hardware uses
interrupts to notify the operating system of status changes.
Modern disk controllers typically
accept a 
\indextext{batch} 
of disk requests at a time and even reorder
them to make most efficient use of the disk arm.
When disks were simpler, operating systems often reordered the
request queue themselves.

Many operating systems have drivers for solid-state disks because they
provide much faster access to data.  But, although a solid-state disk
works very differently from a traditional mechanical disk, both
devices provide block-based interfaces and reading/writing blocks on a
solid-state disk is still more expensive than reading/writing RAM.
The virtio disk makes no distinction between a mechanical disk and
solid-state disk.

Other hardware is surprisingly similar to disks: network device
buffers hold packets, audio device buffers hold sound samples,
graphics card buffers hold video data and command sequences.
High-bandwidth graphics cards, and network cards—often use DMA
as the virtio disk does.

Some drivers dynamically switch between polling and interrupts, because using
interrupts can be expensive, but using polling can introduce delay until the
driver processes an event.  For example, a network driver that receives a
burst of packets may switch from interrupts to polling since it knows that more
packets must be processed and it is less expensive to process them using polling.
Once no more packets need to be processed, the driver may switch back to
interrupts, so that it will be alerted immediately when a new packet arrives.

If a program reads a file, the data for that file is copied twice.  First, it
is copied from the disk to kernel memory by the driver, and then later it is
copied from kernel space to user space by the 
\lstinline{read}
system call.  If the program then sends the data over the network, 
the data is copied twice more: from user space to kernel space and from
kernel space to the network device.  To support applications for which 
efficiency is important (e.g., serving popular images on the Web), operating systems
use special code paths to avoid copies.  As one example,
in real-world operating systems, 
buffers typically match the hardware page size, so that
read-only copies can be mapped into a process's address space
using the paging hardware, without any copying.

\section{Exercises}

\begin{enumerate}
  
\item Add a driver for an Ethernet card.

\end{enumerate}
