%    Sidebar about panic:
% 	panic is the kernel's last resort: the impossible has happened and the
% 	kernel does not know how to proceed.  In xv6, panic does ...
\chapter{Traps and device drivers}
\label{CH:TRAP}

There are three situations in which some event causes the CPU to set
aside its ordinary sequential execution of instructions and forces a
transfer of control to special code that handles the event. One
situation is a system call, when a user program 
executes the {\tt ecall} instruction to ask the kernel to do 
something for it. Another situation is an \indextext{exception}:
an instruction (user or kernel) does something illegal, such as divide
by zero or use an invalid virtual address. The third situation is a
device \indextext{interrupt}, when a device signals that it needs
attention, for example when the disk hardware finishes a read or write
request.

This book uses \indextext{trap} as a generic term for these
situations. Typically whatever code was executing at the time of the
trap will later need to resume, and shouldn't need to be aware that
anything special happened. That is, we often want traps to be
transparent; this is particularly important for interrupts, which the
interrupted code typically doesn't expect. Thus the usual sequence is that
a trap forces a transfer of control into the kernel; the kernel saves
registers and other state so that execution can be resumed; the kernel
executes appropriate handler code (e.g. a system call implementation
or device driver); the kernel restores the saved state and returns
from the trap; and the original code resumes where it left off.

The xv6 kernel handles all traps. This is natural for system calls; it
makes sense for interrupts since isolation demands that user processes
not directly use devices, and because only the kernel has the state
needed for device handling; and it makes sense for exceptions since
xv6 responds to all exceptions from user space by killing the
offending program.

Xv6 trap handling proceeds in four stages: hardware actions taken by
the RISC-V CPU, an assembly ``vector'' that prepares the way for
kernel C code, a C trap handler that decides what to do with the trap,
and the system call or device driver service routine. While
commonality among the three trap types suggests that a kernel could
handle all traps with a single code path, it turns out to be
convenient to have separate assembly vectors and C trap handlers for
three distinct cases: traps from kernel space, traps from user space,
and timer interrupts.

This chapter ends with a discussion of device drivers. Device handling
is a different topic than traps, but is included here because the
kernel's interaction with device hardware is often driven by
interrupts.

\section{RISC-V trap machinery}

RISC-V supports a number of control registers that the kernel writes to
tell the CPU how to handle interrupts, and that the kernel can read
to find out about an interrupt that has occured. The RISC-V documents
contain the full story~\cite{riscv:priv}. {\tt riscv.h}
\lineref{kernel/riscv.h:1} contains definitions that xv6 uses. Here's
an outline of the most important registers:

\begin{itemize}

\item {\bf stvec}: The kernel writes the address of its trap handler
  here; the RISC-V jumps here to handle a trap.

\item {\bf sepc}: When a trap occurs, RISC-V saves the program counter
  here (since the {\tt pc} is then replaced with {\tt stvec}). The
  {\tt sret} (return from trap) instruction copies {\tt sepc} to the
  {\tt pc}. The kernel can write to {\tt sepc} to control where {\tt
    sret} goes.

\item {\bf scause}: The RISC-V puts a number here that describes
the reason for the trap.

\item {\bf sscratch}: The kernel places a value here that comes in
  handy at the very start of a trap handler.

\item {\bf sstatus}: The SIE bit controls whether device interrupts
  are enabled. If the kernel clears SIE, the RISC-V will defer
  device interrupts until the kernel sets SIE. The SPP bit
  indicates whether a trap came from user mode or supervisor
  mode, and controls to what mode {\tt sret} returns.

\end{itemize}

The above relate to interrupts handled in supervisor mode, and they
cannot be read or written in user mode. There is an equivalent set of
control registers for interrupts handled in machine mode; xv6 uses
them only for the special case of timer interrupts.

When it needs to force a trap, the RISC-V hardware does the
following for all trap types (other than timer interrupts):

\begin{enumerate}

\item If the trap is a device interrupt, and the {\tt sstatus} SIE bit
  is clear, don't do any of the following.

\item Disable interrupts by clearing SIE.

\item Copy the {\tt pc} to {\tt sepc}.

\item Save the current mode (user or supervisor) in the SPP bit in {\tt sstatus}.

\item Set {\tt scause} to reflect the interrupt's cause.

\item Set the mode to supervisor.

\item Copy {\tt stvec} to the {\tt pc}.

\item Start executing at the new {\tt pc}.

\end{enumerate}

It is important that the CPU performs all these steps as
a single operation.
Consider if one of these steps were omitted: for example, the CPU
didn't switch program counters. Then, a trap could switch to
supervisor mode while still running user instructions. Those user
instructions could break the user/kernel isolation, for example by
modifying the {\tt satp} register to point to a page table that
allowed accessing all of physical memory. It is thus important that
the kernel specify the trap entry point, and not the user program.

Note that the CPU doesn't switch to the kernel page table, doesn't
switch to a stack in the kernel, and doesn't save any registers other
than the {\tt pc}. The kernel must perform these tasks if necessary.
One reason that the CPU does minimal work during a trap is to provide
flexibility to software; for example, in some situations no page table
switch is needed, which may increase performance.

\section{Traps from kernel space}

When the xv6 kernel is executing on a CPU, two types of traps can
occur: exceptions and device interrupts. The previous section outlined
the CPU's response to such traps.

When the kernel is executing, it arranges that {\tt stvec} point
to the assembler code at {\tt kernelvec}
\lineref{kernel/kernelvec.S:/^kernelvec/}.
Since xv6 is already in the kernel, {\tt kernelvec} can rely
on {\tt satp} being set to the kernel page table, and on the
stack pointer referring to a valid kernel stack.
{\tt kernelvec} saves all registers so that we can
eventually resume the interrupted code without disturbing it.

{\tt kernelvec} saves the registers on the stack of the interrupted
kernel thread, which makes sense because the register values belong to
that thread. This is particularly important if the trap causes a
switch to a different thread -- in that case the trap will actually
return on the stack of the new thread, leaving the interrupted
thread's saved registers safely on its stack.

{\tt kernelvec} jumps to {\tt kerneltrap}
\lineref{kernel/trap.c:/^kerneltrap/} after saving registers.
{\tt kerneltrap} is prepared for two types of traps:
device interrrupts and exceptions. It calls
{\tt devintr}
\lineref{kernel/trap.c:/^devintr/}
to check for and handle the former.
If the trap isn't a device interrupt, it is an exception,
and that is always a fatal error if it occurs in the kernel.

If {\tt kerneltrap} was called due to a timer interrupt, and a
process's kernel thread is running (rather than a scheduler thread),
{\tt kerneltrap} calls {\tt yield} to give other threads a chance to
run. At some point one of those threads will yield, and let our thread
and its {\tt kerneltrap} resume again.
Chapter~\ref{CH:SCHED} explains what happens in {\tt yield}.

When {\tt kerneltrap}'s work is done, it needs to return to whatever
code was interrupted by the trap. Because a {\tt yield} may have
disturbed the saved {\tt sepc} and the saved previous mode in {\tt sstatus},
{\tt kerneltrap} saves them when it starts. It now restores those
control registers and returns to {\tt kernelvec}
\lineref{kernel/kernelvec.S:/call.kerneltrap$/}.
{\tt kernelvec} pops the saved registers from the stack and
executes {\tt sret}, which copies {\tt sepc} to {\tt pc}
and resumes the interrupted kernel code.

It's worth thinking through how the trap return happens if
{\tt kerneltrap} called {\tt yield} due to a timer interrupt.

Xv6 sets a CPU's {\tt stvec} to {\tt kernelvec} when that CPU
enters the kernel from user space; you can see this in {\tt usertrap}
\lineref{kernel/trap.c:/stvec.*kernelvec/}.
There's a window of time when the kernel is executing
but {\tt stvec} has the wrong value, and it's crucial that device
interrupts be disabled during that window.
Luckily the RISC-V always disables interrupts when it starts
to take a trap, and xv6 doesn't enable them again until
after it sets {\tt stvec}.

\section{Traps from user space}

A trap may occur while executing in user space if the
user program makes a
system call ({\tt ecall} instruction), does something
illegal, or if a device interrupts.
The high-level path of a trap from user space is
{\tt uservec}
\lineref{kernel/trampoline.S:/^uservec/},
then {\tt usertrap}
\lineref{kernel/trap.c:/^usertrap/};
and when returning,
{\tt usertrapret}
\lineref{kernel/trap.c:/^usertrapret/}
and then
{\tt userret}
\lineref{kernel/trampoline.S:/^uservec/}.

Traps from user code are more challenging than from the kernel, since
{\tt satp} points to a user page table that doesn't map the 
kernel, and the stack pointer may contain an invalid or even malicious
value.

Because the RISC-V hardware doesn't switch page tables during a trap,
we need the user page table to include a mapping for the trap vector
that {\tt stvec} points to. Further, the trap vector must switch {\tt
  satp} to point to the kernel page table, and in order to avoid a
crash, the vector code must be mapped at the same address in the
kernel page table as in the user page table.

Xv6 satisfies these constraints with a \indextext{trampoline} page
that contains the trap vector code. Xv6 maps the trampoline page at
the same virtual address in the kernel page table and in every user
page table. This virtual address is \indexcode{TRAMPOLINE} (as we saw
in Figure~\ref{fig:as} and in Figure~\ref{fig:xv6_layout}). The
trampoline contents are set in {\tt trampoline.S},
and (when executing user code) {\tt stvec} is set to
{\tt uservec}
\lineref{kernel/trampoline.S:/^uservec/}.

When {\tt uservec} starts, every register contains a value owned by
the interrupting code. But {\tt uservec} needs to be able to modify
some registers in order to set {\tt satp} and generate addresses at
which to save the registers. RISC-V provides a helping hand in the
form of the {\tt sscratch} register. The {\tt csrrw} instruction at
the start of {\tt uservec} swaps the contents of {\tt a0} and {\tt
  sscratch}. Now the user code's {\tt a0} is saved; {\tt uservec} has
one register ({\tt a0}) to play with; and {\tt a0} contains whatever
value the kernel previously placed in {\tt sscratch}.

{\tt uservec}'s next task is to save the user registers. Before
entering user space, the kernel sets {\tt sscratch} to point to a
per-process {\tt trapframe} that (among other things) has space to
save all the user registers
\lineref{kernel/proc.h:/^struct.trapframe/}. Because {\tt satp} still
refers to the user page table, {\tt uservec} needs the trapframe to be
mapped in the user address space. When creating each process, xv6
allocates a page for the process's trapframe, and arranges for it
always to be mapped at user virtual address {\tt TRAPFRAME}, which is
just below {\tt TRAMPOLINE}. The process's {\tt p->tf} points to the
trapframe, though at its physical address where it can be accessed
through the kernel page table.

Thus after swapping {\tt a0} and {\tt sscratch}, {\tt a0}
holds a pointer to the current process's trapframe.
{\tt uservec} now saves all user registers there,
including the user's {\tt a0}, read from {\tt sscratch}.

The {\tt trapframe} contains pointers to the current process's
kernel stack, the current CPU's hartid, the address of {\tt usertrap},
and the address of the kernel page table. {\tt uservec}
retrieves these values, switches {\tt satp} to the kernel page table,
and calls {\tt usertrap}.

The job of {\tt usertrap}, like {\tt kerneltrap} is to determine
the cause of the trap, process it, and return
\lineref{kernel/trap.c:/^usertrap/}.
As mentioned above, it first changes {\tt stvec} to process
traps from kernel mode in {\tt kernelvec}.
It saves the {\tt sepc}, again because there might be a
process switch in {\tt usertrap} that could cause {\tt sepc}
to be overwritten.
If the trap is a system call, {\tt syscall} handles it;
if a device interrupt, {\tt devintr};
otherwise it's an exception, and the kernel kills the
faulting process.
The system call path adds four to the saved user {\tt pc}
because RISC-V, in the case of a system call,
leaves the program pointer pointing to the {\tt ecall} instruction.
On the way out, {\tt usertrap} checks if the process has been
killed or should yield the CPU (if this trap is a timer interrupt).

The first step in returning to user space is the call to {\tt usertrapret}
\lineref{kernel/trap.c:/^usertrapret/}.
This function sets up the RISC-V control registers to prepare for a
future trap from user space. This involves changing {\tt stvec}
to refer to {\tt uservec}, preparing the trapframe fields that
{\tt uservec} relies on, and setting {\tt sepc} to the previously
saved user program counter. At the end, {\tt usertrapret}
calls {\tt userret} on the trampoline page that is mapped in
both user and kernel page tables; the reason is that assembly
code in {\tt userret} will switch page tables.

{\tt usertrapret}'s call to {\tt userret} passes a pointer to the process's user
page table in {\tt a0} and {\tt TRAPFRAME} in {\tt a1}
\lineref{kernel/trampoline.S:/^userret/}.
{\tt userret} switches {\tt satp} to the process's user page table.
Recall that the user page table maps both the trampoline page
and {\tt TRAPFRAME}, but nothing else from the kernel.
Again, the fact that the trampoline page is mapped at the same
virtual address in user and kernel page tables is what allows
{\tt uservec} to keep executing after changing {\tt satp}.
{\tt trapret} copies the trapframe's saved user {\tt a0} to {\tt sscratch}
in preparation for a later swap with with TRAPFRAME.
From this point on, the only data {\tt userret} can use is
the register contents and the content of the trapframe.
Next {\tt userret} restores saved user registers from the trapframe,
does a final swap of {\tt a0} and {\tt sscratch} to restore the
user {\tt a0} and save {\tt TRAPFRAME} for the next trap,
and uses {\tt sret} to return to user space.

\section{Timer interrupts}

Xv6 uses timer interrupts to maintain its clock and to enable it to
switch among compute-bound processes; the {\tt yield} calls in {\tt
  usertrap} and {\tt kerneltrap} cause this switching. Timer
interrupts come from special hardware attached to the RISC-V that xv6
programs to interrupt periodically.

RISC-V requires that timer interrupts be taken in machine mode, not
supervisor mode. RISC-V machine mode executes without paging, and with
a separate set of control registers, so it's not practical to run
ordinary xv6 kernel code in machine mode. As a result, xv6 handles
timer interrupts completely separately from the trap mechanism laid out
above.

Code in {\tt start}
\lineref{kernel/start.c:/receive.timer/}
sets up to receive timer interrupts.
Part of the job is to program the CLINT hardware (core-local interruptor)
to generate an interrupt after a certain delay.
Another part is to set up a scratch area, analogous to trapframe,
for the timer interrupt handler to save registers in and find
the address of the CLINT registers.
Finally, {\tt start} sets {\tt mtvec} to {\tt timervec} and
enables timer interrupts.

A timer interrupt can occur at any point when user or kernel code is
executing; there's no way for the kernel to disable timer interrupts
during critical operations. Thus the timer interrupt handler must do
its job in a way guaranteed not to disturb interrupted kernel code.
The basic strategy is for the timer interrupt to ask the RISC-V to
raise a ``software interrupt'' and immediately return. The RISC-V
delivers software interrupts to the kernel with the ordinary trap
mechanism, and allows the kernel to disable them. The code to
handle the software interrupt generated by a timer interrupt can be
seen in {\tt devintr} \lineref{kernel/trap.c:/machine-mode.timer/}.

The machine-mode timer interrupt vector is {\tt timervec} 
\lineref{kernel/kernelvec.S:/^timervec/}.
It saves a few registers in the scratch area prepared by {\tt start},
tells the CLINT when to generate the next timer interrupt,
asks the RISC-V to raise a software interrupt,
restores registers, and returns.
There's no C code involved in a timer interrupt.

\section{Code: Calling system calls}

Chapter~\ref{CH:FIRST} ended with 
\indexcode{initcode.S}
invoking a system call.
Let's look at that again
\lineref{user/initcode.S:/SYS_exec/}.
The process pushed the arguments
for an 
\indexcode{exec}
call on the process's stack, and put the
system call number in
\texttt{a7}.
The system call numbers match the entries in the syscalls array,
a table of function pointers
\lineref{kernel/syscall.c:/syscalls/}.
The \lstinline{ecall} instruction traps into the kernel
and executes {\tt uservec},
{\tt usertrap}, and then {\tt syscall}, as we saw above.

\indexcode{Syscall}
\lineref{kernel/syscall.c:/^syscall/} 
loads the system call number from the trapframe, which
contains the saved
\texttt{a7},
and indexes into the system call table.
For the first system call, 
\texttt{a7}
contains the value 
\indexcode{SYS_exec}
\lineref{kernel/syscall.h:/SYS_exec/},
and
\lstinline{syscall}
will call the 
\lstinline{SYS_exec}'th 
entry of the system call table, which corresponds to calling
\lstinline{sys_exec}.

\lstinline{Syscall}
records the return value of the system call function in
\lstinline{p->tf->a0}.
When the system call returns to user space,
\lstinline{userret}
will load the values from
\indexcode{p->tf}
into the machine registers
and return to user space
using
\lstinline{sret}.
Thus, when 
\lstinline{exec}
returns in user space, it will return in \lstinline{a0} the value
that the system call handler returned
\lineref{kernel/syscall.c:/a0 = syscalls/}.
System calls conventionally return negative numbers to indicate
errors, and zero or positive numbers for success.
If the system call number is invalid,
\indexcode{syscall}
prints an error and returns $-1$.

\section{Code: System call arguments}

Later chapters will examine the implementation of
particular system calls.
This chapter is concerned with the mechanisms for system calls.
There is one bit of mechanism left: finding the system call arguments.

The arguments themselves are relatively easy to find, since the
C calling convention on RISC-V specifies that arguments be
passed in registers.
During a system call, these registers (the saved user registers)
are available in the trapframe, {\tt p->tf}.
The helper functions
\lstinline{argint},
\lstinline{argaddr},
\lstinline{argptr},
and
\lstinline{argfd}
retrieve the 
\textit{n} 'th 
system call
argument, as either an integer, pointer, a string, or a file descriptor.
They all call {\tt fetcharg} to retrieve one of the saved
user registers
\lineref{kernel/syscall.c:/^fetcharg/}.

Some system calls pass pointers as arguments, and the kernel must use
those pointers to read or write user memory. The {\tt exec} system
call, for example, passes the kernel an array of pointers
referring to string arguments in user space.
These pointers pose
two challenges. First, the user program may be buggy or malicious, and
may pass the kernel an invalid pointer or a pointer intended to trick
the kernel into accessing kernel memory instead of user memory.
Second, the xv6 kernel page table mappings are not the same as the
user page table mappings, so the kernel cannot use ordinary
instructions to load or store from user-supplied addresses.

A number of kernel functions need to perform safe reads from user space; {\tt
  fetchstr} is an example \lineref{kernel/syscall.c:/^fetchstr/}.
File system calls such as
{\tt exec} use {\tt fetchstr} to retrieve string arguments from user
space.
\lstinline{fetchstr} calls \lstinline{copyinstr},
which knows how to look up virtual addresses in a user page
table and turn them into addresses the kernel can use.

\indexcode{copyinstr}
\lineref{kernel/vm.c:/^copyinstr/} copies up to \lstinline{max} bytes to
\lstinline{dst} from virtual address \lstinline{srcva} in the page
table \lstinline{pagetable}.  It uses {\tt walkaddr}
(which calls {\tt walk}) to walk the page table in software to
determine the physical address \lstinline{pa0} for \lstinline{srcva}.
Since the kernel maps all physical RAM addresses to the same
kernel virtual address,
{\tt copyinstr} can directly copy string bytes from {\tt pa0} to {\tt dst}.
{\tt walkaddr} 
\lineref{kernel/vm.c:/^walkaddr/}
validates that the user-supplied virtual address is part of
the process's valid address space, so user programs
cannot trick the kernel into reading other memory.
A similar function, {\tt copyout}, copies data from the
kernel to a user-supplied address.

\section{Device drivers}

A
\indextext{driver}
is the code in an operating system that manages a particular device:
it tells the device hardware to perform operations,
configures the device to generate interrupts when done,
and handles the resulting interrupts.
Driver code can be tricky to write
because a driver executes concurrently with the device that it manages.  In
addition, the driver must understand the device's interface (e.g., which I/O
ports do what), and that interface can be complex and poorly documented.

Devices that need attention from the operating system can usually be
configured to generate interrupts, one of types of trap. Typically the
device driver's initialization code will tell the hardware that the
driver wants interrupts. On the RISC-V, there is also a hardware unit
called the PLIC that the driver must configure to forward interrupt
requests to the RISC-V CPUs. A final piece of glue is that the
kernel trap handling code must be able to recognize when the device
has raised an interrupt and call the driver's interrupt handler;
in xv6, this dispatch happens in {\tt devintr} \lineref{kernel/trap.c:/^devintr/}.

The disk driver provides a good example.  The disk driver copies data
from and to the disk, as requested by the file system.
Disk hardware traditionally presents the data on the
disk as a numbered sequence of 512-byte 
\textit{blocks} 
\index{block}
(also called 
\textit{sectors}): 
\index{sector}
sector 0 is the first 512 bytes, sector 1 is the next, and so on. The block size
that an operating system uses for its file system maybe different than the
sector size that a disk uses, but typically the block size is a multiple of the
sector size. Xv6 holds copies of blocks that it has read into memory
in objects of type
\lstinline{struct buf}
\lineref{kernel/buf.h:/^struct.buf/}.
The
data stored in this structure is sometimes out of sync with the disk: it might have
not yet been read in from disk (the disk is working on it but hasn't returned
the sector's content yet), or it might have been updated by software
but not yet written to the disk.
The driver must ensure that the rest of xv6 doesn't get confused when
a {\tt struct buf}
is out of sync with the disk.

\section{Code: Disk driver}

Xv6 has a disk driver for \indextext{virtio} disks. The
virtio standard is a standard for devices for kernels that run on a
virtual machine~\cite{virtio}.
It defines the interactions between a guest kernel
and the virtual machine monitor for virtual devices emulating network
devices, disk devices, etc.  Xv6 runs on \texttt{qemu} and
\texttt{qemu} supports a virtio disk device.

The xv6 file system uses blocks of 1024 bytes,
or \lstinline{BSIZE}
\lineref{kernel/fs.h:/BSIZE/},
which is twice the virtio disk's sector size.
Each {\tt struct buf}
\lineref{kernel/buf.h:1}
contains BSIZE bytes,
and the driver always reads and writes the disk in these units.
The
\lstinline{dev}
and
\lstinline{blockno}
fields of {\tt struct buf} indicate which disk
device and block number the buffer refers to. The
\lstinline{data}
field is an in-memory copy of the disk sectors that correspond to that block.
The
\lstinline{flags} field
tracks the relationship between memory and disk:
the
\indexcode{B_VALID}
flag means that
\lstinline{data}
has been read from the disk, and
\indexcode{B_DIRTY} 
means that
\lstinline{data}
needs to be written out.

Xv6 talks to the virtio disk controller
through memory-mapped registers.  These registers start at address
\lstinline{0x10001000} in the kernel adddress space (see
Figure~\ref{fig:xv6_layout}).  The driver uses ordinary load and
store instructions to interact with the device, but the addresses
have special meaning.  For example, reading from the address
\lstinline{0x10001000 + VIRTIO_MMIO_VERSION}
\lineref{kernel/virtio.h:/VERSION/} returns the version of the virtio
standard that the device supports
\lineref{kernel/virtio\_disk.c:/MMIO_VERSION/}.  The driver uses the
macro \lstinline{R} to read/write registers on the disk controller
\lineref{kernel/virtio\_disk.c:/R/}.

The kernel initializes the disk driver at boot time by calling
\indexcode{virtio_disk_init}
\lineref{kernel/virtio\_disk.c:/^virtio_disk_init/} from
\indexcode{main} \lineref{kernel/main.c:/virtio_disk_init/}.
\lstinline{virtio_disk_init} negotiates the simplest features with the
disk \linerefs{kernel/virtio\_disk.c:/features/,/=.features/} and sets
up one queue for disk requests \linerefs{kernel/virtio\_disk.c:/QUEUE_SEL/,/free\[/}.  The driver can have
\lstinline{NUM} outstanding requests to the disk.

Disk accesses typically take milliseconds, a long time for a
CPU.  Therefore, xv6 lets another process run on the CPU after a
disk I/O request and arranges to receive an interrupt when the disk
operation has completed.  
\indexcode{plic_init}, which is called from \lstinline{main}, programs
the \indextext{Platform Level Interrupt Controller (PLIC)}~\cite{riscv:priv} to
accept interrupts from the disk.  It enables interrupts for
\lstinline{VIRTIO0_IRQ} \lineref{kernel/plic.c:/^plicinit/}.  If the
disk generates an interrupt, \lstinline{devintr}
\lineref{kernel/trap.c:/^devintr/} will see that
\lstinline{VIRTIO0_IRQ} was enabled and invoke
\lstinline{virtio_disk_intr}.

After this setup, the disk is not used again until the buffer cache calls
\indexcode{virtio_disk_rw}
\lineref{kernel/virtio\_disk.c:/^virtio_disk_rw/},
which updates a locked buffer
as indicated by the flags.
If
\indexcode{B_DIRTY}
is set,
\lstinline{virtio_disk_rw}
writes the buffer
to the disk; if
\indexcode{B_VALID}
is not set,
\indexcode{virtio_disk_rw}
reads the buffer from the disk.
To add a request to the queue of requests, the driver must find 3
descriptors to describe the request.  The first descriptor
describes the operation (e.g., read/write).  The second descriptor
records the address of the data to be read or written. The third
descriptor is where the device will post the result status of the
request.  \lstinline{Virtio_Disk_Rw} starts by finding 3 free descriptors.  If
it cannot find 3 descriptors, it means that the queue of requests is
full, and it waits until a previously issued request completes, freeing
up descriptors.   If it finds 3 descriptors, the drivers fills them
in \linerefs{kernel/virtio\_disk.c:/buf0.type/,/.next.=.0/}.

The virtio disk supports \textit{Direct Memory Access
  (DMA)}\index{DMA}, which allows the device to read/write physical
memory.  The driver gives the device the physical address of the
buffer's data \lineref{kernel/virtio\_disk.c:/b->data/} and the device
copies directly to or from main memory.  DMA is faster and more
efficient than programmed I/O and is less taxing for the CPU's memory
caches.

Once the driver has filled out the descriptors, it can alert the disk
that a new quests is available.  It does so by writing to
\lstinline{avail}.  The disk monitors \lstinline{avail} and reads the
value in \lstinline{avail[1]} to find out how many new requests it
should process.  To make sure that writes to \lstinline{avail} are not
re-ordered, the driver inserts a memory barrier between them
\lineref{kernel/virtio\_disk.c:/sync_synchronize/}. This guarantees
that if the disk sees a new requests in \lstinline{avail[1]}, it will
also see the correct index in \lstinline{avail[2 + (avail[1]%NUM)]}.

Having posted the request to the disk,
\lstinline{virtio_disk_rw}
must wait for the result.  As discussed above,
polling does not make efficient use of the CPU.
Instead,
\indexcode{virtio_disk_rw}
yields the CPU for other processes by sleeping,
waiting for the interrupt handler to 
record in the buffer's flags that the operation is done
\linerefs{kernel/virtio\_disk.c:/while.*VALID/,/sleep/}.
While this process is sleeping,
xv6 will schedule other processes to keep the CPU busy.

Eventually, the disk will finish its operation and trigger an
interrupt, which will result in a call to
\indexcode{virtio_disk_intr}
to handle it
\lineref{kernel/trap.c:/virtio_disk_intr/}.
The disk posts the requests that it has completed in \lstinline{used}.
\lstinline{Virtio_Disk_Intr}
\lineref{kernel/virtio\_disk.c:/^virtio_disk_intr/}
reads \lstinline{used} to find the \lstinline{id} of
the request that has completed.
Then, \lstinline{virtio_disk_intr}
sets 
\indexcode{B_VALID},
clears
\indexcode{B_DIRTY} for the buffer in the request,
wakes up any process sleeping on the buffer
\linerefs{kernel/virtio\_disk.c:/info\[id\]\.b/,/wakeup/},
and frees up the descriptors that were used for request.
Freeing up a descriptor also wakes up processes
waiting for free descriptors.

The PLIC routes interrupts to the first CPU that claims it.
Thus, cores can process different disk interrupts concurrently.  In
some operating systems, this routing can get sophisticated.  For
example, a network driver might arrange to deliver interrupts for
packets of one network connection to the CPU that is managing
that connection, while interrupts for packets of another connection
are delivered to another CPU.  If some network connections are
short lived while others are long lived and the operating system wants
to keep all CPUs busy to achieve high throughput, this routing
becomes quite complex.

\section{Real world}

Supporting all the devices on a typical computer in its full glory is
much work, because there are many devices, the devices have many
features, and the protocol between device and driver can be complex
and badly documented.
In many operating systems, the drivers account for more code
than the core kernel.

Actual device drivers are far more complex than the disk driver in this chapter,
but the basic ideas are the same:
typically devices are slower than CPU, so the hardware uses
interrupts to notify the operating system of status changes.
Modern disk controllers typically
accept a 
\indextext{batch} 
of disk requests at a time and even reorder
them to make most efficient use of the disk arm.
When disks were simpler, operating systems often reordered the
request queue themselves.

Many operating systems have drivers for solid-state disks because they
provide much faster access to data.  But, although a solid-state disk
works very differently from a traditional mechanical disk, both
devices provide block-based interfaces and reading/writing blocks on a
solid-state disk is still more expensive than reading/writing RAM.
The virtio disk makes no distinction between a mechanical disk and
solid-state disk.

Other hardware is surprisingly similar to disks: network device
buffers hold packets, audio device buffers hold sound samples,
graphics card buffers hold video data and command sequences.
High-bandwidth graphics cards, and network cards often use DMA
as the virtio disk does.

Some drivers dynamically switch between polling and interrupts, because using
interrupts can be expensive, but using polling can introduce delay until the
driver processes an event.  For example, a network driver that receives a
burst of packets may switch from interrupts to polling since it knows that more
packets must be processed and it is less expensive to process them using polling.
Once no more packets need to be processed, the driver may switch back to
interrupts, so that it will be alerted immediately when a new packet arrives.

If a program reads a file, the data for that file is copied twice.  First, it
is copied from the disk to kernel memory by the driver, and then later it is
copied from kernel space to user space by the 
\lstinline{read}
system call.  If the program then sends the data over the network, 
the data is copied twice more: from user space to kernel space and from
kernel space to the network device.  To support applications for which 
efficiency is important (e.g., serving popular images on the Web), operating systems
use special code paths to avoid copies.  As one example,
in real-world operating systems, 
buffers typically match the hardware page size, so that
read-only copies can be mapped into a process's address space
using the paging hardware, without any copying.

\section{Exercises}

\begin{enumerate}
  
\item Add a driver for an Ethernet card.

\end{enumerate}
