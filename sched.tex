% cox and mullender, semaphores.
% 
% process has one thread with two stacks
%  
% pike et al, sleep and wakeup
\chapter{Scheduling}
\label{CH:SCHED}

Any operating system is likely to run with more processes than the
computer has processors, so a plan is needed to time-share the
processors among the processes. Ideally the sharing would be transparent to user
processes.  A common approach is to provide each process
with the illusion that it has its own virtual processor by
\textit{multiplexing}\index{multiplexing}
the processes onto the hardware processors.
This chapter explains how xv6 achieves this multiplexing.
%% 
\section{Multiplexing}
%% 

Xv6 multiplexes by switching each processor from one process to
another in two situations. First, xv6's
\lstinline{sleep}
and
\lstinline{wakeup}
mechanism switches when a process waits
for device or pipe I/O to complete, or waits for a child
to exit, or waits in the
\lstinline{sleep}
system call.
Second, xv6 periodically forces a switch to cope with
processes that compute for long periods without sleeping.
This multiplexing creates the illusion that
each process has its own CPU, just as xv6 uses the memory allocator and hardware
page tables to create the illusion that each process has its own memory.

Implementing multiplexing poses a few challenges. First, how to switch from one
process to another? 
Although the idea of context switching
is simple, the implementation is some of the most opaque code
in xv6. Second, how to force
switches in a way that is transparent to user processes?  Xv6 uses the
standard technique of driving context switches with timer interrupts.
Third, many CPUs may be switching among processes concurrently, and a
locking plan is necessary to avoid races. Fourth, a process's
memory and other resources must be freed when the process exits,
but it cannot do all of this itself
because (for example) it can't free its own kernel stack while still using it.
Finally, each core of a multi-core machine must remember which
process it is executing so that system calls affect the correct
process's kernel state.
Xv6 tries to solve these problems as
simply as possible, but nevertheless the resulting code is
tricky.

xv6 must provide
ways for processes to coordinate among themselves. For example,
a parent process may need to wait for one of its children to
exit, or a process reading a pipe may need to wait for
some other process to write the pipe.
Rather than make the waiting process waste CPU by repeatedly checking
whether the desired event has happened, xv6 allows a process to give
up the CPU and sleep
waiting for an event, and allows another process to wake the first
process up. Care is needed to avoid races that result in
the loss of event notifications.
%% 
\section{Code: Context switching}
%% 

\begin{figure}[t]
\center
\includegraphics[scale=0.5]{fig/switch.eps}
\caption{Switching from one user process to another.  In this example, xv6 runs with one CPU (and thus one scheduler thread).}
\label{fig:switch}
\end{figure}

Figure~\ref{fig:switch} 
outlines the steps involved in switching from one
user process to another:
a user-kernel transition (system
call or interrupt) to the old process's kernel thread,
a context switch to the current CPU's scheduler thread, a context
switch to a new process's kernel thread, and a trap return
to the user-level process.
The xv6 scheduler has a dedicate thread (saved registers and stack)
per CPU because
it is sometimes not safe for it execute on
any process's kernel stack;
we'll see an example in
\lstinline{exit}.
In this section we'll examine the mechanics of switching
between a kernel thread and a scheduler thread.

Switching from one thread to another involves saving the old thread's
CPU registers, and restoring the previously-saved registers of the
new thread; the fact that
the stack pointer and program counter
are saved and restored means that the CPU will switch stacks and
switch what code it is executing.

The function
\lstinline{swtch}\index{swtch@\lstinline{swtch}}
performs the saves and restores for a thread switch.
\lstinline{swtch}
doesn't directly know about threads; it just saves and
restores register sets, called 
\textit{contexts}\index{contexts}.
When it is time for a process to give up the CPU,
the process's kernel thread calls
\lstinline{swtch}
to save its own context and return to the scheduler context.
Each context is represented by a
\lstinline{struct}
\lstinline{context*}
\lineref{kernel/proc.h:/^struct.context/},
a pointer to a structure stored on the kernel stack involved.
\lstinline{Swtch}
takes two arguments:
\lstinline{struct context}\index{struct context@\lstinline{struct context}}
\lstinline{**old}
and
\lstinline{struct}
\lstinline{context}
\lstinline{*new}.
It pushes the current registers onto the stack
and saves the stack pointer in
\lstinline{*old}.
Then
\lstinline{swtch}
copies
\lstinline{new}
to 
\texttt{\%rsp},
pops previously saved registers, and returns.

Let's follow a user process through
\lstinline{swtch} 
into the scheduler.
We saw in Chapter~\ref{CH:TRAP}
that one possibility at the end of each interrupt
is that 
\lstinline{trap}\index{trap@\lstinline{trap}}
calls 
\lstinline{yield}\index{yield@\lstinline{yield}}.
\lstinline{Yield}
in turn calls
\lstinline{sched}\index{sched@\lstinline{sched}},
which calls
\lstinline{swtch}\index{swtch@\lstinline{swtch}}
to save the current context in
\lstinline{proc->context}
and switch to the scheduler context previously saved in 
\lstinline{cpu->scheduler}\index{cpu->scheduler@\lstinline{cpu->scheduler}}
\lineref{kernel/proc.c:/swtch..p/}.

\lstinline{Swtch}
\lineref{kernel/swtch.S:/swtch/}
pushes the register state, creating a context structure
on the current stack.
Only the callee-saved registers need to be saved;
the convention on the x86-64 is that these are
\texttt{\%rbp},
\texttt{\%rbx},
\texttt{\%r11}
through
\texttt{\%r15},
and
\texttt{\%rsp}.
\lstinline{Swtch}
pushes the first 7 explicitly
\linerefs{kernel/swtch.S:/push..rbp/,/push..r15/};
it saves the last implicitly as the
\lstinline{struct}
\lstinline{context*}
written to
\lstinline{*old} 
\lineref{kernel/swtch.S:/mov..rsp/}.
There is one more important register:
the program counter 
\texttt{\%eip}.
It has already been saved on the stack by the
\lstinline{call}
instruction that invoked
\lstinline{swtch}.
Having saved the old context,
\lstinline{swtch}
is ready to restore the new one.
It moves the pointer to the new context
into the stack pointer
\lineref{kernel/swtch.S:/mov..rsi/}.
The new stack has the same form as the old one that
\lstinline{swtch}\index{swtch@\lstinline{swtch}}
just left—the new stack
\textit{was}
the old one in a previous call to
\lstinline{swtch} —\c
so 
\lstinline{swtch}
can invert the sequence to restore the new context.
It pops the values for
\texttt{\%r15}
through
\texttt{\%r11},
and
\texttt{\%rbx},
and
\texttt{\%rbp},
and then returns
\linerefs{kernel/swtch.S:/pop/,/ret/}.
Because 
\lstinline{swtch}
has changed the stack pointer, the values restored
and the instruction address returned to
are the ones from the new context.

In our example, 
\lstinline{sched}\index{sched@\lstinline{sched}}
called
\lstinline{swtch}\index{swtch@\lstinline{swtch}}
to switch to
\lstinline{cpu->scheduler}\index{cpu->scheduler@\lstinline{cpu->scheduler}},
the per-CPU scheduler context.
That context had been saved by 
\lstinline{scheduler} 's
call to
\lstinline{swtch}
\lineref{kernel/proc.c:/swtch.&.c/}.
When the
\lstinline{swtch}\index{swtch@\lstinline{swtch}}
we have been tracing returns,
it returns not to
\lstinline{sched}
but to 
\lstinline{scheduler}\index{scheduler@\lstinline{scheduler}},
and its stack pointer points at the current CPU's
scheduler stack.
%% 
\section{Code: Scheduling}
%% 

The last section looked at the low-level details of
\lstinline{swtch}\index{swtch@\lstinline{swtch}};
now let's take 
\lstinline{swtch}
as a given and examine 
switching from a process through the scheduler to another process.
A process
that wants to give up the CPU must
acquire the process table lock
\lstinline{ptable.lock}\index{ptable.lock@\lstinline{ptable.lock}},
release any other locks it is holding,
update its own state
(\lstinline{proc->state}),
and then call
\lstinline{sched}\index{sched@\lstinline{sched}}.
\lstinline{Yield}
\lineref{kernel/proc.c:/^yield/}
follows this convention, as do
\lstinline{sleep}\index{sleep@\lstinline{sleep}}
and
\lstinline{exit}\index{exit@\lstinline{exit}},
which we will examine later.
\lstinline{Sched}
double-checks those conditions
\linerefs{kernel/proc.c:/if..holding/,/running/}
and then an implication of those conditions:
since a lock is held, the CPU should be
running with interrupts disabled.
Finally,
\lstinline{sched}\index{sched@\lstinline{sched}}
calls
\lstinline{swtch}\index{swtch@\lstinline{swtch}}
to save the current context in 
\lstinline{proc->context}
and switch to the scheduler context in
\lstinline{cpu->scheduler}\index{cpu->scheduler@\lstinline{cpu->scheduler}}.
\lstinline{Swtch}
returns on the scheduler's stack
as though
\lstinline{scheduler}\index{scheduler@\lstinline{scheduler}} 's
\lstinline{swtch}
had returned
\lineref{kernel/proc.c:/swtch.&.c/}.
The scheduler continues the 
\lstinline{for}
loop, finds a process to run, 
switches to it, and the cycle repeats.

We just saw that xv6 holds
\lstinline{ptable.lock}\index{ptable.lock@\lstinline{ptable.lock}}
across calls to
\lstinline{swtch}:
the caller of
\lstinline{swtch}\index{swtch@\lstinline{swtch}}
must already hold the lock, and control of the lock passes to the
switched-to code.  This convention is unusual with locks; usually
the thread that acquires a lock is also responsible for
releasing the lock, which makes it easier to reason about correctness.
For context switching it is necessary to break this convention because
\lstinline{ptable.lock}\index{ptable.lock@\lstinline{ptable.lock}}
protects invariants on the process's
\lstinline{state}
and
\lstinline{context}
fields that are not true while executing in
\lstinline{swtch}.
One example of a problem that could arise if
\lstinline{ptable.lock}
were not held during
\lstinline{swtch}\index{swtch@\lstinline{swtch}}:
a different CPU might decide
to run the process after 
\lstinline{yield}\index{yield@\lstinline{yield}}
had set its state to
\lstinline{RUNNABLE},
but before 
\lstinline{swtch}
caused it to stop using its own kernel stack.
The result would be two CPUs running on the same stack,
which cannot be right.

A kernel thread always gives up its
processor in
\lstinline{sched} 
and always switches to the same location in the scheduler, which
(almost) always switches to some kernel thread that previously called
\lstinline{sched}. 
Thus, if one were to print out the line numbers where xv6 switches
threads, one would observe the following simple pattern:
\lineref{kernel/proc.c:/swtch.&.c/},
\lineref{kernel/proc.c:/swtch..p/},
\lineref{kernel/proc.c:/swtch.&.c/},
\lineref{kernel/proc.c:/swtch..p/},
and so on.  The procedures in which this stylized switching between
two threads happens are sometimes referred to as 
\textit{coroutines}\index{coroutines}; 
in this example,
\lstinline{sched}\index{sched@\lstinline{sched}}
and
\lstinline{scheduler}\index{scheduler@\lstinline{scheduler}}
are co-routines of each other.

There is one case when the scheduler's call to
\lstinline{swtch}\index{swtch@\lstinline{swtch}}
does not end up in
\lstinline{sched}\index{sched@\lstinline{sched}}.
We saw this case in Chapter~\ref{CH:MEM}: when a
new process is first scheduled, it begins at
\lstinline{forkret}\index{forkret@\lstinline{forkret}}
\lineref{kernel/proc.c:/^forkret/}.
\lstinline{Forkret}
exists to release the 
\lstinline{ptable.lock}\index{ptable.lock@\lstinline{ptable.lock}};
otherwise, the new process could start at
\lstinline{trapret}.

\lstinline{Scheduler}
\lineref{kernel/proc.c:/^scheduler/} 
runs a simple loop:
find a process to run, run it until it yields, repeat.
\lstinline{scheduler}\index{scheduler@\lstinline{scheduler}}
holds
\lstinline{ptable.lock}\index{ptable.lock@\lstinline{ptable.lock}}
for most of its actions,
but releases the lock (and explicitly enables interrupts)
once in each iteration of its outer loop.
This is important for the special case in which this CPU
is idle (can find no
\lstinline{RUNNABLE}
process).
If an idling scheduler looped with
the lock continuously held, no other CPU that
was running a process could ever perform a context
switch or any process-related system call,
and in particular could never mark a process as
\lstinline{RUNNABLE}\index{RUNNABLE@\lstinline{RUNNABLE}}
so as to break the idling CPU out of its scheduling loop.
The reason to enable interrupts periodically on an idling
CPU is that there might be no
\lstinline{RUNNABLE}
process because processes (e.g., the shell) are
waiting for I/O;
if the scheduler left interrupts disabled all the time,
the I/O would never arrive.

The scheduler
loops over the process table
looking for a runnable process, one that has
\lstinline{p->state} 
\lstinline{==}
\lstinline{RUNNABLE}.
Once it finds a process, it sets the per-CPU current process
variable
\lstinline{proc},
switches to the process's page table with
\lstinline{switchuvm}\index{switchuvm@\lstinline{switchuvm}},
marks the process as
\lstinline{RUNNING},
and then calls
\lstinline{swtch}\index{swtch@\lstinline{swtch}}
to start running it
\linerefs{kernel/proc.c:/Switch.to/,/swtch/}.

One way to think about the structure of the scheduling code is
that it arranges to enforce a set of invariants about each process,
and holds
\lstinline{ptable.lock}\index{ptable.lock@\lstinline{ptable.lock}}
whenever those invariants are not true.
One invariant is that if a process is
\lstinline{RUNNING},
a timer interrupt's
\lstinline{yield}\index{yield@\lstinline{yield}}
must be able to switch away from the process;
this means that the CPU registers must hold the process's register values
(i.e. they aren't actually in a
\lstinline{context}),
\texttt{\%cr3}
must refer to the process's pagetable,
\texttt{\%esp}
must refer to the process's kernel stack so that
\lstinline{swtch}
can push registers correctly, and
\lstinline{proc}
must refer to the process's
\lstinline{proc[]}
slot.
Another invariant is that if a process is
\lstinline{RUNNABLE}\index{RUNNABLE@\lstinline{RUNNABLE}},
an idle CPU's
\lstinline{scheduler}\index{scheduler@\lstinline{scheduler}}
must be able to run it;
this means that 
\lstinline{p->context}\index{p->context@\lstinline{p->context}}
must hold the process's kernel thread variables,
that no CPU is executing on the process's kernel stack,
that no CPU's
\texttt{\%cr3}
refers to the process's page table,
and that no CPU's
\lstinline{proc}
refers to the process.

Maintaining the above invariants is the reason why xv6 acquires 
\lstinline{ptable.lock}\index{ptable.lock@\lstinline{ptable.lock}}
in one thread (often in
\lstinline{yield)}
and releases the lock in a different thread
(the scheduler thread or another next kernel thread).
Once the code has started to modify a running process's state
to make it
\lstinline{RUNNABLE},
it must hold the lock until it has finished restoring
the invariants: the earliest correct release point is after
\lstinline{scheduler}
stops using the process's page table and clears
\lstinline{proc}.
Similarly, once 
\lstinline{scheduler}
starts to convert a runnable process to
\lstinline{RUNNING},
the lock cannot be released until the kernel thread
is completely running (after the
\lstinline{swtch},
e.g. in
\lstinline{yield}).

\lstinline{ptable.lock}\index{ptable.lock@\lstinline{ptable.lock}}
protects other things as well:
allocation of process IDs and free process table slots,
the interplay between
\lstinline{exit}\index{exit@\lstinline{exit}}
and
\lstinline{wait}\index{wait@\lstinline{wait}},
the machinery to avoid lost wakeups (see next section),
and probably other things too.
It might be worth thinking about whether the 
different functions of
\lstinline{ptable.lock}
could be split up, certainly for clarity and perhaps
for performance.
%% 
\section{Code: mycpu and myproc}
%% 

As we saw in Chapter~\ref{CH:TRAP},
xv6 maintains a
\lstinline{struct cpu}\index{struct cpu@\lstinline{struct cpu}}
for each processor, which records
the process currently running
on the processor (if any),
the processor's unique hardware identifier
(\lstinline{apicid}),
and some other information.
The function
\lstinline{getmycpu}\index{getmycpu@\lstinline{getmycpu}}
\lineref{kernel/proc.c:/^getmycpu/}
returns the current processor's
\lstinline{struct cpu}.
\lstinline{getmycpu}
does this by reading the processor
identifier from the local APIC hardware and looking through
the array of
\lstinline{struct cpu}
for an entry with that identifier.
The function 
\lstinline{seginit}
\lineref{kernel/vm.c:/^seginit/}
programs the register
\lstinline{MSR_GS_KERNBASE}
to contain a pointer to the core's
\lstinline{struct cpu}.
The function
\lstinline{mycpu}\index{mycpu@\lstinline{mycpu}}
\lineref{kernel/proc.c:/^mycpu/}
returns that
value using
\texttt{\%gs}.
Using
\texttt{\%gs}
avoids every call to
\lstinline{mycpu}
having to loop
through the array
\lstinline{cpus},
as
\lstinline{getmycpu}
does.

The return value of
\lstinline{getmycpu}
and
\lstinline{mycpu}
are fragile: if the timer were to interrupt and cause
the thread to be moved to a different processor, the
return value would no longer be correct.
To avoid this problem, xv6 requires that callers of
\lstinline{getmycpu}
disable interrupts, and only enable
them after they finish using the returned
\lstinline{struct cpu}.

The function
\lstinline{myproc}\index{myproc@\lstinline{myproc}}
\lineref{kernel/proc.c:/^myproc/}
returns the
\lstinline{struct proc}
pointer
for the process that is running on the current processor.
\lstinline{myproc}
disables interrupts, invokes
\lstinline{mycpu},
fetches the current process pointer
(\lstinline{c->proc})
out of the
\lstinline{struct cpu},
and then enables interrupts.
If there is no process running, because the the caller is
executing in
\lstinline{scheduler},
\lstinline{myproc}
returns zero.
The return value of
\lstinline{myproc}
is safe to use even if interrupts are enabled:
if a timer interrupt moves the calling process to a
different processor, its
\lstinline{struct proc}
pointer will stay the same.
%% 
\section{Sleep and wakeup}
%% 

Scheduling and locks help conceal the existence of one process
from another,
but so far we have no abstractions that help
processes intentionally interact.
Sleep and wakeup fill that void, allowing one process to 
sleep waiting for an event and another process to wake it up
once the event has happened.
Sleep and wakeup are often called 
\textit{sequence coordination}\index{sequence coordination}
or 
\textit{conditional synchronization}\index{conditional synchronization}
mechanisms, and there are many other similar mechanisms
in the operating systems literature.

To illustrate what we mean, let's consider a
simple producer/consumer queue.
XXX This queue is similar to the one that feeds commands from processes
to the virtio driver
(see Chapter~\ref{CH:TRAP}), but abstracts away all
IDE-specific code.
The queue allows one process to send a nonzero pointer
to another process.
If there were only one sender and one receiver,
and they executed on different CPUs,
and the compiler didn't optimize too agressively,
this implementation would be correct:
\begin{lstlisting}[]
  100	struct q {
  101	  void *ptr;
  102	};
  103	
  104	void*
  105	send(struct q *q, void *p)
  106	{
  107	  while(q->ptr != 0)
  108	    ;
  109	  q->ptr = p;
  110	}
  111	
  112	void*
  113	recv(struct q *q)
  114	{
  115	  void *p;
  116	
  117	  while((p = q->ptr) == 0)
  118	    ;
  119	  q->ptr = 0;
  120	  return p;
  121	}
\end{lstlisting}
\lstinline{Send}
loops until the queue is empty
\lstinline{ptr} "" (
\lstinline{==}
\lstinline{0)}
and then puts the pointer
\lstinline{p}
in the queue.
\lstinline{Recv}
loops until the queue is non-empty
and takes the pointer out.
When run in different processes,
\lstinline{send}
and
\lstinline{recv}
both modify
\lstinline{q->ptr},
but
\lstinline{send}
only writes the pointer when it is zero
and
\lstinline{recv}
only writes the pointer when it is nonzero,
so no updates are lost.

The implementation above 
is expensive.  If the sender sends
rarely, the receiver will spend most
of its time spinning in the 
\lstinline{while}
loop hoping for a pointer.
The receiver's CPU could find more productive work than
\textit{busy waiting}\index{busy waiting}
by repeatedly 
\textit{polling}\index{polling}
\lstinline{q->ptr}.
Avoiding busy waiting requires
a way for the receiver to yield the CPU
and resume only when 
\lstinline{send}
delivers a pointer.

Let's imagine a pair of calls, 
\lstinline{sleep}\index{sleep@\lstinline{sleep}}
and
\lstinline{wakeup}\index{wakeup@\lstinline{wakeup}},
that work as follows.
\lstinline{Sleep(chan)}
sleeps on the arbitrary value
\lstinline{chan}\index{chan@\lstinline{chan}},
called the 
\textit{wait channel}\index{wait channel}.
\lstinline{Sleep}
puts the calling process to sleep, releasing the CPU
for other work.
\lstinline{Wakeup(chan)}
wakes all processes sleeping on
\lstinline{chan}
(if any), causing their
\lstinline{sleep}
calls to return.
If no processes are waiting on
\lstinline{chan},
\lstinline{wakeup}
does nothing.
We can change the queue implementation to use
\lstinline{sleep}
and
\lstinline{wakeup}:
\begin{lstlisting}[]
  201	void*
  202	send(struct q *q, void *p)
  203	{
  204	  while(q->ptr != 0)
  205	    ;
  206	  q->ptr = p;
  207	  wakeup(q);  /* wake recv */
  208	}
  209	
  210	void*
  211	recv(struct q *q)
  212	{
  213	  void *p;
  214	
  215	  while((p = q->ptr) == 0)
  216	    sleep(q);
  217	  q->ptr = 0;
  218	  return p;
  219	}
\end{lstlisting}

\begin{figure}[t]
\center
\includegraphics[scale=0.5]{fig/deadlock.eps}
\caption{Example lost wakeup problem}
\label{fig:deadlock}
\end{figure}

\lstinline{Recv}
now gives up the CPU instead of spinning, which is nice.
However, it turns out not to be straightforward to design
\lstinline{sleep}
and 
\lstinline{wakeup}
with this interface without suffering
from what is known as the ``lost wake-up'' problem (see 
Figure~\ref{fig:deadlock}).
Suppose that
\lstinline{recv}
finds that
\lstinline{q->ptr}
\lstinline{==}
\lstinline{0} 
on line 215.
While
\lstinline{recv}
is between lines 215 and 216,
\lstinline{send}
runs on another CPU:
it changes
\lstinline{q->ptr}
to be nonzero and calls
\lstinline{wakeup},
which finds no processes sleeping and thus does nothing.
Now
\lstinline{recv}
continues executing at line 216:
it calls
\lstinline{sleep}
and goes to sleep.
This causes a problem:
\lstinline{recv}
is asleep waiting for a pointer
that has already arrived.
The next
\lstinline{send}
will wait for 
\lstinline{recv}
to consume the pointer in the queue,
at which point the system will be 
\textit{deadlocked}\index{deadlocked}.

The root of this problem is that the
invariant that
\lstinline{recv}
only sleeps when
\lstinline{q->ptr}
\lstinline{==}
\lstinline{0}
is violated by 
\lstinline{send}
running at just the wrong moment.
One incorrect way of protecting the invariant would be to modify the code for
\lstinline{recv}
as follows:
\begin{lstlisting}[]
  300	struct q {
  301	  struct spinlock lock;
  302	  void *ptr;
  303	};
  304	
  305	void*
  306	send(struct q *q, void *p)
  307	{
  308	  acquire(&q->lock);
  309	  while(q->ptr != 0)
  310	    ;
  311	  q->ptr = p;
  312	  wakeup(q);
  313	  release(&q->lock);
  314	}
  315	
  316	void*
  317	recv(struct q *q)
  318	{
  319	  void *p;
  320	
  321	  acquire(&q->lock);
  322	  while((p = q->ptr) == 0)
  323	    sleep(q);
  324	  q->ptr = 0;
  325	  release(&q->lock);
  326	  return p;
  327	}
\end{lstlisting}
One might hope that this version of
\lstinline{recv}
would avoid the lost wakeup because the lock prevents
\lstinline{send}
from executing between lines 322 and 323.
It does that, but it also deadlocks:
\lstinline{recv}
holds the lock while it sleeps,
so the sender will block forever waiting for the lock.

We'll fix the preceding scheme by changing
\lstinline{sleep} 's
interface:
the caller must pass the lock to
\lstinline{sleep}
so it can release the lock after
the calling process is marked as asleep and waiting on the
sleep channel.
The lock will force a concurrent
\lstinline{send}
to wait until the receiver has finished putting itself to sleep,
so that the
\lstinline{wakeup}
will find the sleeping receiver and wake it up.
Once the receiver is awake again
\lstinline{sleep}\index{sleep@\lstinline{sleep}}
reacquires the lock before returning.
Our new correct scheme is useable as follows:
\begin{lstlisting}[]
  400	struct q {
  401	  struct spinlock lock;
  402	  void *ptr;
  403	};
  404	
  405	void*
  406	send(struct q *q, void *p)
  407	{
  408	  acquire(&q->lock);
  409	  while(q->ptr != 0)
  410	    ;
  411	  q->ptr = p;
  412	  wakeup(q);
  413	  release(&q->lock);
  414	}
  415	
  416	void*
  417	recv(struct q *q)
  418	{
  419	  void *p;
  420	
  421	  acquire(&q->lock);
  422	  while((p = q->ptr) == 0)
  423	    sleep(q, &q->lock);
  424	  q->ptr = 0;
  425	  release(&q->lock);
  426	  return p;
  427	}
\end{lstlisting}

The fact that
\lstinline{recv}
holds
\lstinline{q->lock}
prevents 
\lstinline{send}
from trying to wake it up between 
\lstinline{recv} 's
check of
\lstinline{q->ptr}
and its call to
\lstinline{sleep}.
We need
\lstinline{sleep}
to atomically release
\lstinline{q->lock}
and put the receiving process to sleep.

A complete sender/receiver implementation would also sleep
in
\lstinline{send}
when waiting for a receiver to consume
the value from a previous
\lstinline{send}.
%% 
\section{Code: Sleep and wakeup}
%% 

Let's look at the implementation of
\lstinline{sleep}\index{sleep@\lstinline{sleep}}
\lineref{kernel/proc.c:/^sleep/}
and
\lstinline{wakeup}\index{wakeup@\lstinline{wakeup}}
\lineref{kernel/proc.c:/^wakeup/}.
The basic idea is to have
\lstinline{sleep}
mark the current process as
\lstinline{SLEEPING}\index{SLEEPING@\lstinline{SLEEPING}}
and then call
\lstinline{sched}\index{sched@\lstinline{sched}}
to release the processor;
\lstinline{wakeup}
looks for a process sleeping on the given wait channel
and marks it as 
\lstinline{RUNNABLE}\index{RUNNABLE@\lstinline{RUNNABLE}}.
Callers of
\lstinline{sleep}
and
\lstinline{wakeup}
can use any mutually convenient number as the channel.
Xv6 often uses the address
of a kernel data structure involved in the waiting.

\lstinline{Sleep}
\lineref{kernel/proc.c:/^sleep/}
begins with a few sanity checks:
there must be a current process
\lineref{kernel/proc.c:/p.==.0/}
and
\lstinline{sleep}
must have been passed a lock
\linerefs{kernel/proc.c:/lk == 0/,/sleep.without/}.
Then 
\lstinline{sleep}
acquires 
\lstinline{ptable.lock}\index{ptable.lock@\lstinline{ptable.lock}}
\lineref{kernel/proc.c:/sleeplock1/}.
Now the process going to sleep holds both
\lstinline{ptable.lock}
and
\lstinline{lk}.
Holding
\lstinline{lk}
was necessary in the caller (in the example,
\lstinline{recv}):
it
ensured that no other process (in the example,
one running
\lstinline{send})
could start a call to
\lstinline{wakeup(chan)}.
Now that
\lstinline{sleep}
holds
\lstinline{ptable.lock},
it is safe to release
\lstinline{lk}:
some other process may start a call to
\lstinline{wakeup(chan)},
but
\lstinline{wakeup}\index{wakeup@\lstinline{wakeup}}
will not run until it can acquire
\lstinline{ptable.lock}\index{ptable.lock@\lstinline{ptable.lock}},
so it must wait until
\lstinline{sleep}
has finished putting the process to sleep,
keeping the
\lstinline{wakeup}
from missing the
\lstinline{sleep}.

There is a minor complication: if 
\lstinline{lk}
is equal to
\lstinline{&ptable.lock},
then
\lstinline{sleep}
would deadlock trying to acquire it as
\lstinline{&ptable.lock}
and then release it as
\lstinline{lk}.
In this case,
\lstinline{sleep}
considers the acquire and release
to cancel each other out
and skips them entirely
\lineref{kernel/proc.c:/sleeplock0/}.
For example,
\lstinline{wait}
\lineref{kernel/proc.c:/^wakeup\(/}
calls
\lstinline{sleep}
with 
\lstinline{&ptable.lock}.

Now that
\lstinline{sleep}
holds
\lstinline{ptable.lock}
and no others,
it can put the process to sleep by recording
the sleep channel,
changing the process state,
and calling
\lstinline{sched}
\lineref{kernel/proc.c:/chan.=.chan/,/sched/}.

At some point later, a process will call
\lstinline{wakeup(chan)}.
\lstinline{Wakeup}
\lineref{'kernel/proc.c:/^wakeup\(/}'
acquires
\lstinline{ptable.lock}\index{ptable.lock@\lstinline{ptable.lock}}
and calls
\lstinline{wakeup1}\index{wakeup1@\lstinline{wakeup1}},
which does the real work.
It is important that
\lstinline{wakeup}\index{wakeup@\lstinline{wakeup}}
hold the
\lstinline{ptable.lock}
both because it is manipulating process states
and because, as we just saw,
\lstinline{ptable.lock}
makes sure that
\lstinline{sleep}
and
\lstinline{wakeup}
do not miss each other.
\lstinline{Wakeup1}
is a separate function because
sometimes the scheduler needs to
execute a wakeup when it already
holds the 
\lstinline{ptable.lock};
we will see an example of this later.
\lstinline{Wakeup1}
\lineref{kernel/proc.c:/^wakeup1/}
loops over the process table.
When it finds a process in state
\lstinline{SLEEPING}\index{SLEEPING@\lstinline{SLEEPING}}
with a matching
\lstinline{chan}\index{chan@\lstinline{chan}},
it changes that process's state to
\lstinline{RUNNABLE}\index{RUNNABLE@\lstinline{RUNNABLE}}.
The next time the scheduler runs, it will
see that the process is ready to be run.

Xv6 code always calls
\lstinline{wakeup}
while holding the lock that guards the sleep condition;
in the example above that lock is
\lstinline{q->lock}.
Strictly speaking it is sufficient if
\lstinline{wakeup}
always follows the
\lstinline{acquire}
(that is, one could call
\lstinline{wakeup}
after the
\lstinline{release}).
Why do the locking rules for 
\lstinline{sleep}
and
\lstinline{wakeup}
ensure a sleeping process won't miss a wakeup it needs?
The sleeping process holds either
the lock on the condition or the
\lstinline{ptable.lock} 
or both from a point before it checks the condition
to a point after it is marked as sleeping.
If a concurrent thread causes the condition to be true,
that thread must either hold the lock on the condition before the
sleeping thread acquired it, or after the sleeping
thread released it in
\lstinline{sleep}.
If before, the sleeping thread must have seen the new condition
value, and decided to sleep anyway, so it doesn't matter
if it misses the wakeup.
If after, then the earliest the waker could acquire the
lock on the condition is after
\lstinline{sleep}
acquires
\lstinline{ptable.lock},
so that
\lstinline{wakeup} 's
acquisition of
\lstinline{ptable.lock}
must wait until
\lstinline{sleep}
has completely finished putting the sleeper to sleep.
Then 
\lstinline{wakeup}
will see the sleeping process and wake it up
(unless something else wakes it up first).

It is sometimes the case that multiple processes are sleeping
on the same channel; for example, more than one process
reading from a pipe.
A single call to 
\lstinline{wakeup}
will wake them all up.
One of them will run first and acquire the lock that
\lstinline{sleep}
was called with, and (in the case of pipes) read whatever
data is waiting in the pipe.
The other processes will find that, despite being woken up,
there is no data to be read.
From their point of view the wakeup was ``spurious,'' and
they must sleep again.
For this reason sleep is always called inside a loop that
checks the condition.

No harm is done if two uses of sleep/wakeup accidentally
choose the same channel: they will see spurious wakeups,
but looping as described above will tolerate this problem.
Much of the charm of sleep/wakeup is that it is both
lightweight (no need to create special data
structures to act as sleep channels) and provides a layer
of indirection (callers need not know which specific process
they are interacting with).
%% 
\section{Code: Pipes}
%% 
The simple queue we used earlier in this chapter
was a toy, but xv6 contains two real queues
that use
\lstinline{sleep}
and
\lstinline{wakeup}
to synchronize readers and writers.
One is in the virtio driver: a process adds a disk request to a queue and then
calls
\lstinline{sleep}.
The virtio interrupt handler uses
\lstinline{wakeup}
to alert the process that its request has completed.

A more complex example is the implementation of pipes.
We saw the interface for pipes in Chapter~\ref{CH:UNIX}:
bytes written to one end of a pipe are copied
in an in-kernel buffer and then can be read out
of the other end of the pipe.
Future chapters will examine the file descriptor support
surrounding pipes, but let's look now at the
implementations of 
\lstinline{pipewrite}\index{pipewrite@\lstinline{pipewrite}}
and
\lstinline{piperead}\index{piperead@\lstinline{piperead}}.

Each pipe
is represented by a 
\lstinline{struct pipe}\index{struct pipe@\lstinline{struct pipe}},
which contains
a 
\lstinline{lock}
and a 
\lstinline{data}
buffer.
The fields
\lstinline{nread}
and
\lstinline{nwrite}
count the number of bytes read from
and written to the buffer.
The buffer wraps around:
the next byte written after
\lstinline{buf[PIPESIZE-1]}
is 
\lstinline{buf[0]}.
The counts do not wrap.
This convention lets the implementation
distinguish a full buffer 
\lstinline{nwrite} "" (
\lstinline{==}
\lstinline{nread}+PIPESIZE)
from an empty buffer
\lstinline{nwrite} "" (
\lstinline{==}
\lstinline{nread}),
but it means that indexing into the buffer
must use
\lstinline{buf[nread}
\lstinline{%}
\lstinline{PIPESIZE]}
instead of just
\lstinline{buf[nread]} 
(and similarly for
\lstinline{nwrite}).
Let's suppose that calls to
\lstinline{piperead}
and
\lstinline{pipewrite}
happen simultaneously on two different CPUs.

\lstinline{Pipewrite}
\lineref{kernel/pipe.c:/^pipewrite/}
begins by acquiring the pipe's lock, which
protects the counts, the data, and their
associated invariants.
\lstinline{Piperead}
\lineref{kernel/pipe.c:/^piperead/}
then tries to acquire the lock too, but cannot.
It spins in
\lstinline{acquire}
\lineref{kernel/spinlock.c:/^acquire/}
waiting for the lock.
While
\lstinline{piperead}
waits,
\lstinline{pipewrite}
loops over the bytes being written
\lstinline{addr[0]},
\lstinline{addr[1]},
\&...,
\lstinline{addr[n-1]}
adding each to the pipe in turn
\lineref{kernel/pipe.c:/nwrite\+\+/}.
During this loop, it could happen that
the buffer fills
\lineref{kernel/pipe.c:/pipewrite-full/}.
In this case, 
\lstinline{pipewrite}
calls
\lstinline{wakeup}
to alert any sleeping readers to the fact
that there is data waiting in the buffer
and then sleeps on
\lstinline{&p->nwrite}
to wait for a reader to take some bytes
out of the buffer.
\lstinline{Sleep}
releases 
\lstinline{p->lock}
as part of putting
\lstinline{pipewrite} 's
process to sleep.

Now that
\lstinline{p->lock}
is available,
\lstinline{piperead}
manages to acquire it and enters its critical section:
it finds that
\lstinline{p->nread}
\lstinline{!=}
\lstinline{p->nwrite}
\lineref{kernel/pipe.c:/pipe-empty/}
\lstinline{pipewrite} "" (
went to sleep because
\lstinline{p->nwrite}
\lstinline{==}
\lstinline{p->nread}+PIPESIZE
\lineref{kernel/pipe.c:/pipewrite-full/})
so it falls through to the 
\lstinline{for}
loop, copies data out of the pipe
\lineref{kernel/pipe.c:/piperead-copy/,/^..}/},
and increments 
\lstinline{nread}
by the number of bytes copied.
That many bytes are now available for writing, so
\lstinline{piperead}
calls
\lstinline{wakeup}
\lineref{kernel/pipe.c:/piperead-wakeup/}
to wake any sleeping writers
before it returns to its caller.
\lstinline{Wakeup}
finds a process sleeping on
\lstinline{&p->nwrite},
the process that was running
\lstinline{pipewrite}
but stopped when the buffer filled.
It marks that process as
\lstinline{RUNNABLE}\index{RUNNABLE@\lstinline{RUNNABLE}}.

The pipe code uses separate sleep channels for reader and writer
(
\lstinline{p->nread}
and
\lstinline{p->nwrite});
this might make the system more efficient in the unlikely
event that there are lots of
readers and writers waiting for the same pipe.
The pipe code sleeps inside a loop checking the
sleep condition; if there are multiple readers
or writers, all but the first process to wake up
will see the condition is still false and sleep again.
%% 
\section{Code: Wait, exit, and kill}
%% 
\lstinline{Sleep}
and
\lstinline{wakeup}
can be used for many kinds of waiting.
An interesting example, seen in Chapter~\ref{CH:UNIX},
is the
\lstinline{wait}\index{wait@\lstinline{wait}}
system call that a parent process uses to wait for a child to exit.
When a child exits, it does not die immediately.
Instead, it switches to the
\lstinline{ZOMBIE}\index{ZOMBIE@\lstinline{ZOMBIE}}
process state until the parent calls
\lstinline{wait}
to learn of the exit.
The parent is then responsible for freeing the
memory associated with the process 
and preparing the
\lstinline{struct proc}\index{struct proc@\lstinline{struct proc}}
for reuse.
If the parent exits before the child, the 
\lstinline{init}
process
adopts the child and waits for it, so that
every child has a parent to clean up after it.

An implementation challenge is
the possibility of races between
parent and child
\lstinline{wait}
and
\lstinline{exit},
as well as
\lstinline{exit}
and
\lstinline{exit}.
\lstinline{Wait}
begins by
acquiring 
\lstinline{ptable.lock}.
Then it scans the process table
looking for children.
If 
\lstinline{wait}
finds that the current process has children
but that none have exited,
it calls
\lstinline{sleep}
to wait for one of them to exit
\lineref{kernel/proc.c:/wait-sleep/}
and scans again.
Here,
the lock being released in 
\lstinline{sleep}
is
\lstinline{ptable.lock},
the special case we saw above.

\lstinline{Exit}
acquires
\lstinline{ptable.lock}
and then wakes up any process sleeping on a wait
channel equal to the current process's parent
\lstinline{proc}
\lineref{kernel/proc.c:/wakeup1.curproc->parent/};
if there is such a process, it will be the parent in
\lstinline{wait}.
This may look premature, since 
\lstinline{exit}\index{exit@\lstinline{exit}}
has not marked the current process as a
\lstinline{ZOMBIE}
yet, but it is safe:
although
\lstinline{wakeup}
may cause the parent to run,
the loop in
\lstinline{wait}
cannot run until
\lstinline{exit}
releases 
\lstinline{ptable.lock}
by calling
\lstinline{sched}
to enter the scheduler,
so
\lstinline{wait}
can't look at
the exiting process until after
\lstinline{exit}
has set its state to
\lstinline{ZOMBIE}
\lineref{kernel/proc.c:/state.=.ZOMBIE/}.
Before exit yields the processor,
it reparents all of
the exiting process's children,
passing them to the
\lstinline{initproc}
\linerefs{kernel/proc.c:/Pass.abandoned/,/wakeup1/+2}.
Finally,
\lstinline{exit}
calls
\lstinline{sched}\index{sched@\lstinline{sched}}
to relinquish the CPU.

If the parent process was sleeping in
\lstinline{wait},
the scheduler will eventually run it.
The call to
\lstinline{sleep}
returns holding
\lstinline{ptable.lock};
\lstinline{wait}
rescans the process table
and finds the exited child with
\lstinline{state}
\lstinline{==}
\lstinline{ZOMBIE}.
\lineref{kernel/proc.c:/state.==.ZOMBIE/}.
It records the child's
\lstinline{pid}
and then cleans up the 
\lstinline{struct} 
\lstinline{proc},
freeing the memory associated
with the process
\lineref{kernel/proc.c:/pid.=.p..pid/,/killed.=.0/}.

The child process could have done most
of the cleanup during
\lstinline{exit},
but it is important that the parent 
process be the one to free
\lstinline{p->kstack}\index{p->kstack@\lstinline{p->kstack}} 
and 
\lstinline{p->pgdir}\index{p->pgdir@\lstinline{p->pgdir}}:
when the child runs
\lstinline{exit},
its stack sits in the memory allocated as
\lstinline{p->kstack} 
and it uses its own pagetable.
They can only be freed after the child process has
finished running for the last time by calling
\lstinline{swtch}\index{swtch@\lstinline{swtch}}
(via
\lstinline{sched}).
This is one reason that the scheduler procedure runs on its
own stack rather than on the stack of the thread
that called
\lstinline{sched}.

While
\lstinline{exit} 
allows a process to terminate itself,
\lstinline{kill}
\lineref{kernel/proc.c:/^kill/} 
lets one process request that another be terminated.
It would be too complex for
\lstinline{kill}
to directly destroy the victim process, since the victim
might be executing on another CPU or sleeping
while midway through updating kernel data structures.
To address these challenges, 
\lstinline{kill}
does very little: it just sets the victim's
\lstinline{p->killed}
and, if it is sleeping, wakes it up.
Eventually the victim will enter or leave the kernel,
at which point code in
\lstinline{trap}
will call
\lstinline{exit}
if
\lstinline{p->killed}
is set.
If the victim is running in user space, it will soon enter
the kernel by making a system call or because the timer (or
some other device) interrupts.

If the victim process is in
\lstinline{sleep},
the call to
\lstinline{wakeup}
will cause the victim process to return from
\lstinline{sleep}.
This is potentially dangerous because 
the condition being waiting for may not be true.
However, xv6 calls to
\lstinline{sleep}
are always wrapped in a
\lstinline{while}
loop that re-tests the condition after
\lstinline{sleep}
returns.
Some calls to
\lstinline{sleep}
also test
\lstinline{p->killed}
in the loop, and abandon the current activity if it is set.
This is only done when such abandonment would be correct.
For example, the pipe read and write code
\lineref{kernel/pipe.c:/myproc..-\>killed/} 
returns if the killed flag is set; eventually the
code will return back to trap, which will again
check the flag and exit.

Some xv6 
\lstinline{sleep}
loops do not check
\lstinline{p->killed} 
because the code is in the middle of a multi-step
system call that should be atomic.
The virtio driver
\lineref{ide.c:/sleep.b/} 
is an example: it does not check
\lstinline{p->killed}
because a disk operation may be one of a set of
writes that are all needed in order for the file system to
be left in a correct state.
To avoid the complication of cleaning up after a partial operation, xv6 delays
the killing of a process that is in the virtio driver until some point later when
it is easy to kill the process (e.g., when the complete file system operation
has completed and the process is about to return to user space).
%% 
\section{Real world}
%% 

The xv6 scheduler implements a simple scheduling policy, which runs each process
in turn.  This policy is called
\textit{round robin}\index{round robin}.
Real operating systems implement more sophisticated policies that, for example,
allow processes to have priorities.  The idea is that a runnable high-priority process
will be preferred by the scheduler over a runnable low-priority process.   These
policies can become complex quickly because there are often competing goals: for
example, the operating might also want to guarantee fairness and
high throughput.  In addition, complex policies may lead to unintended
interactions such as
\textit{priority inversion}\index{priority inversion}
and 
\textit{convoys}\index{convoys}.
Priority inversion can happen when a low-priority and high-priority process
share a lock, which when acquired by the low-priority process can prevent the
high-priority process from making progress.  A long convoy can form when many
high-priority processes are waiting for a low-priority process that acquires a
shared lock; once a convoy has formed it can persist for long time.
To avoid these kinds of problems additional mechanisms are necessary in
sophisticated schedulers.

\lstinline{Sleep}
and
\lstinline{wakeup}
are a simple and effective synchronization method,
but there are many others.
The first challenge in all of them is to
avoid the ``lost wakeups'' problem we saw at the
beginning of the chapter.
The original Unix kernel's
\lstinline{sleep}
simply disabled interrupts,
which sufficed because Unix ran on a single-CPU system.
Because xv6 runs on multiprocessors,
it adds an explicit lock to
\lstinline{sleep}.
FreeBSD's
\lstinline{msleep}
takes the same approach.
Plan 9's 
\lstinline{sleep}
uses a callback function that runs with the scheduling
lock held just before going to sleep;
the function serves as a last minute check
of the sleep condition, to avoid lost wakeups.
The Linux kernel's
\lstinline{sleep}
uses an explicit process queue instead of
a wait channel; the queue has its own internal lock.

Scanning the entire process list in
\lstinline{wakeup}
for processes with a matching
\lstinline{chan}
is inefficient.  A better solution is to
replace the
\lstinline{chan}
in both
\lstinline{sleep}
and
\lstinline{wakeup}
with a data structure that holds
a list of processes sleeping on that structure.
Plan 9's
\lstinline{sleep}
and
\lstinline{wakeup}
call that structure a rendezvous point or
\lstinline{Rendez}.
Many thread libraries refer to the same
structure as a condition variable;
in that context, the operations
\lstinline{sleep}
and
\lstinline{wakeup}
are called
\lstinline{wait}
and
\lstinline{signal}.
All of these mechanisms share the same
flavor: the sleep condition is protected by
some kind of lock dropped atomically during sleep.

The implementation of
\lstinline{wakeup}
wakes up all processes that are waiting on a particular channel, and it might be
the case that many processes are waiting for that particular channel.   The
operating system will schedule all these processes and they will race to check
the sleep condition.  Processes that behave in this way are sometimes called a
\textit{thundering herd}\index{thundering herd},
and it is best avoided.
Most condition variables have two primitives for
\lstinline{wakeup}:
\lstinline{signal},
which wakes up one process, and
\lstinline{broadcast},
which wakes up all processes waiting.

Semaphores are another common coordination
mechanism.
A semaphore is an integer value with two operations,
increment and decrement (or up and down).
It is aways possible to increment a semaphore,
but the semaphore value is not allowed to drop below zero:
a decrement of a zero semaphore sleeps until
another process increments the semaphore,
and then those two operations cancel out.
The integer value typically corresponds to a real
count, such as the number of bytes available in a pipe buffer
or the number of zombie children that a process has.
Using an explicit count as part of the abstraction
avoids the ``lost wakeup'' problem:
there is an explicit count of the number
of wakeups that have occurred.
The count also avoids the spurious wakeup
and thundering herd problems.

Terminating processes and cleaning them up introduces much complexity in xv6.
In most operating systems it is even more complex, because, for example, the
victim process may be deep inside the kernel sleeping, and unwinding its
stack requires much careful programming.  Many operating systems unwind the stack
using explicit mechanisms for exception handling, such as
\lstinline{longjmp}.
Furthermore, there are other events that can cause a sleeping process to be
woken up, even though the event it is waiting for has not happened yet.  For
example, when a Unix process is sleeping, another process may send a 
\lstinline{signal}\index{signal@\lstinline{signal}}
to it.  In this case, the
process will return from the interrupted system call with the value -1 and with
the error code set to EINTR. The application can check for these values and
decide what to do.  Xv6 doesn't support signals and this complexity doesn't arise.

Xv6's support for
\lstinline{kill}
is not entirely satisfactory: there are sleep loops
which probably should check for
\lstinline{p->killed}.
A related problem is that, even for 
\lstinline{sleep}
loops that check
\lstinline{p->killed},
there is a race between 
\lstinline{sleep}
and
\lstinline{kill};
the latter may set
\lstinline{p->killed}
and try to wake up the victim just after the victim's loop
checks
\lstinline{p->killed}
but before it calls
\lstinline{sleep}.
If this problem occurs, the victim won't notice the
\lstinline{p->killed}
until the condition it is waiting for occurs. This may be quite a bit later
(e.g., when the virtio driver returns a disk block that the victim is waiting for) or never
(e.g., if the victim is waiting from input from the console, but the user
doesn't type any input).

A real operating system would find free
\lstinline{proc}
structures with an explicit free list
in constant time instead of the linear-time search in
\lstinline{allocproc};
xv6 uses the linear scan for simplicity.
%% 
\section{Exercises}
%% 

1. Sleep has to check
\lstinline{lk != &ptable.lock}
to avoid a deadlock
\linerefs{kernel/proc.c:/sleeplock0/,/^..}/}.
Suppose the special case were eliminated by
replacing
\begin{lstlisting}[]
if(lk != &ptable.lock){
  acquire(&ptable.lock);
  release(lk);
}
\end{lstlisting}
with
\begin{lstlisting}[]
release(lk);
acquire(&ptable.lock);
\end{lstlisting}
Doing this would break
\lstinline{sleep}.
How?

2. Most process cleanup could be done by either
\lstinline{exit}
or
\lstinline{wait},
but we saw above that
\lstinline{exit}
must not free
\lstinline{p->stack}.
It turns out that
\lstinline{exit}
must be the one to close the open files.
Why?
The answer involves pipes.

3. Implement semaphores in xv6.
You can use mutexes but do not use sleep and wakeup.
Replace the uses of sleep and wakeup in xv6
with semaphores.  Judge the result.

4. Fix the race mentioned above between
\lstinline{kill}
and 
\lstinline{sleep},
so that a
\lstinline{kill}
that occurs after the victim's sleep loop checks
\lstinline{p->killed}
but before it calls
\lstinline{sleep}
results in the victim abandoning the current system call.
% Answer: a solution is to to check in sleep if p->killed is set before setting
% the processes's state to sleep. 

5. Design a plan so that every sleep loop checks 
\lstinline{p->killed}
so that, for example, a process that is in the virtio driver can return quickly from the while loop
if another kills that process.
% Answer: this is difficult.  Moderns Unixes do this with setjmp and longjmp and very carefully programming to clean any partial state that the interrupted systems call may have built up.

6. Design a plan that uses only one context switch when switching from one user
process to another.  This plan involves running the scheduler procedure on the
kernel stack of the user process, instead of the dedicated scheduler stack.  The
main challenge is to clean up a user process correctly.  Measure the performance
benefit of avoiding one context switch.
% Answer: maybe keep the current design but create a fast path for when there is
% a runnable user process available.

7. Modify xv6 to turn off a processor when it is idle and just spinning in the
loop in
\lstinline{scheduler}.
(Hint: look at the x86
\lstinline{HLT}
instruction.)

8. The lock
\lstinline{p->lock}
protects many invariants, and when looking at a particular piece of xv6 code that
is protected by
\lstinline{p->lock},
it can be difficult to figure out which invariant is being enforced.  Design a
plan that is more clean by perhaps splitting
\lstinline{p->lock}
in several locks.
% Answer: don't know.
